\documentclass[12pt,oneside]{book}

\input{preamble-apunts.tex}
\input{commands-analisi.tex}

\title{Real Analysis}
\author{Arnau Mas}
\date{2019}

\begin{document}
\maketitle

\frontmatter
\pagestyle{plain}
These are notes gathered during the subject \emph{Anàlisi Real i Funcional} as taught by Joan Orobitg between September 2019 and January 2020.

\mainmatter

\part{Measure Theory}
\chapter{Measure spaces}
\begin{definition}[\( \sigma \)-algebra]
	We say a family of subsets \( \A \subseteq \P(X) \) of a set \( X \) is a \( \sigma \)-algebra over it if
	\begin{points}
	\item \( \emptyset, X \in \A \),
	\item \( \A \) is closed under countable unions, i.e. if there is a countable set \( \{ A_i \}_{i \in \N} \subseteq \A \) then \( \bigcup_{i \in \N} A_i \in \A \),
	\item \( \A \) is closed under countable intersections, i.e. if there is a countable set \( \{ A_i \}_{i \in \N} \subseteq \A \) then \( \bigcap_{i \in \N} A_i \in \A \),
	\item \( \A \) is closed under complements, i.e. if \( A \in \A \) then \( A^c \in \A \).
	\end{points}
\end{definition}
Notice that if a collection of subsets is closed under countable unions and under complements then it is also closed under intersections since by De Morgan's laws a countable intersection is the complement of the union of complements. Similarly if it is closed under intersections and complements it is closed under unions. So when showing that a certain collection is a \( \sigma \)-algebra it is enough to show that is closed under one of unions or intersections.

\begin{example}
	The following are all examples of \( \sigma \)-algebras.
	\begin{points}
	\item For any set \( X \), \( \P(X) \) is a \( \sigma \)-algebra called the \emph{discrete \( \sigma \)-algebra}. It is the \emph{finest} \( \sigma \)-algebra since any other possible \( \sigma \)-algebra over \( X \) is contained in it.
	\item On the other hand, the \emph{coarsest} \( \sigma \)-algebra over any set \( X \) is simply \( \{ \emptyset, X \} \), meaning any other possible \( \sigma \)-algebra contains it. It is called the \emph{trivial \( \sigma \)-algebra}.
	\item If \( \A_1 \) and \( \A_2 \) are \( \sigma \)-algebras over a set \( X \) then so is \( \A_1 \cap \A_2 \).
	\item Given a family of subsets \( S \subseteq \P(X) \) then the \( \sigma \)-algebra generated by it is the intersection of all \( \sigma \)-algebras that contain it and it is the smallest \( \sigma \)-algebra that contains \( S \). We write it \( \sigma(S) \).
	\item The \emph{Borel \( \sigma \)-algebra} over a topological space \( X \) is the \( \sigma \)-algebra generated by the open sets of \( X \), written \( \B(X) \). Since a closed set is the complement of an open set the family of closed sets also generates the Borel \( \sigma \)-algebra.
	\end{points}
\end{example}
The pair formed by a set and its \( \sigma \)-algebra is called a \emph{measurable space}.

\begin{definition}[Measure]
	Let \( (X,\A) \) be a measurable space. A measure is a map \( \mu \colon A \to [0,\infty] \) such that the following are true
	\begin{points}
	\item \( \mu(\emptyset) = 0 \).
	\item If \( \{ A_i \}_{i \in \N} \subseteq \A \) is a family of pairwise disjoint sets of finite measure then
		\begin{equation*}
			\mu\left(\bigcup_{i \in \N} A_i\right) = \sum_{i \in \N} \mu(A_i). 
		\end{equation*}
	\end{points}
\end{definition}
A measurable space equipped with a measure is called a \emph{measure space}.

\begin{example}
	The following are all examples of measures.
	\begin{points}
	\item Consider a measurable space \( X \) with the discrete \( \sigma \)-algebra. Then, for any subset \( A \subseteq X \) we define \( \mu(A) = \abs{A} \) if \( A \) is finite and \( \mu(A) = \infty \) if \( A \) is infinite. This is the \emph{counting measure}, for obvious reasons.
	\item On a finite measurable space \( X \) with the discrete \( \sigma \)-algebra we define for any subset \( A \subseteq X \)
		\begin{equation*}
			\mu(A) = \frac{\abs{A}}{\abs{X}}.
		\end{equation*}
		This is a special case of a probability measure since \( \mu(X) = 1 \). In fact a probability is exactly a measure satisfying \( \mu(X) = 1 \).
	\item On a measurable space \( X \) with any \( \sigma \)-algebra fix a point \( x \in X \) and define \( \mu(A) = 1 \) if \( x \in A \) and \( \mu(A) = 0 \) otherwise. This is called the \emph{Dirac measure}.
	\end{points}
\end{example}

\chapter{The Lebesgue measure}
A question worth asking is whether any measurable space can be made into a measure space. Caratheódory's extension theorem gives an affirmative answer to the question provided we give a starting point for the measure. Roughly speaking the starting point consists of specifying the measure of a collection of subsets, subject to some requirements, which can then be extended to a measure on the whole of the \( \sigma \)-algebra. In this section we explore a particular case of this construction on \( \R^n \) which is known as the \emph{Lebesgue measure}.

The main motivation behind the Lebesgue measure is to rigorously generalise the idea of length ---in the case of \( \R \)---, area ---in the case of \( \R^2 \)--- and volume ---in the case of \( \R^n \)--- to arbitrary dimension and for as many subsets as possible. The starting point will be the rectangles (segments in \( \R \), rectangles in \( \R^2 \), prisms in \( \R^3 \)--- for which their volume is clear: it is simply the product of the length of the sides. 

\section{The Lebesgue exterior measure}
As we said, the starting point for the construction will be rectangles. Let's lay down the precise definitions.
\begin{definition}[Interval]
	An \emph{interval} \( I \) is a subset of \( \R \) with the property that if \( a, b \in I \) then \( c \in I \) whenever \( a < c < b \). It can be shown that if an interval is bounded then it must be one of \( [a,b] \), \( (a,b) \), \( [a,b) \) or \( (a,b] \). \( a \) and \( b \) are called the \emph{endpoints} of the interval and we will write \( \langle a,b \rangle \) for any interval with endpoints \( a \) and \( b \).

The length of an interval with endpoints \( a \) and \( b \) is defined to be \( \abs{b - a} \).
\end{definition}

\begin{definition}[Rectangle]
	An (\( n \)-dimensional) \emph{rectangle} \( R \) is the product of \( n \) intervals, that is
	\begin{equation*}
		R = \langle a_1, b_1 \rangle \times \cdots \times \langle a_n, b_n \rangle.
	\end{equation*}
	The \emph{volume} of a rectangle is defined to be
	\begin{equation*}
		v(R) = \abs{b_1 - a_1} \cdot \cdots \cdot \abs{b_n - a_n}.
	\end{equation*}
\end{definition}
These two definitions of (hopefully) clear concepts are perhaps needlessly fussy but it pays to be precise in the beginning.

\begin{definition}[Exterior measure]
	We define the \emph{Lebesgue exterior measure} or simply \emph{exterior measure} as
	\begin{equation*}
		\inf \left\{ \sum_{j = 1}^{\infty} v(R_j) \mid A \subseteq \bigcup_{j = 1}^{\infty}R_j \text{, }R_j\text{ rectangles} \right\}.
	\end{equation*}
	We will denote it by \( \ext{A} \).
\end{definition}
The intuition behind the exterior measure is as follows: given any set, cover it with rectangles and add up their volumes. Then try to refine the covering by acheiving less total area. The infimimum of the volumes of all possible covers is the exterior measure. In two dimensions this describes trying to literally cover the set by a patchwork of rectangles finer and finer that approximates the area of the set in question.

Also, given that the volume of a rectangle is always positive, the set we are taking the infimum of is bounded below by zero and so its infimimum always exists and is non-negative. Thus the exterior measure exists for any set. 

\begin{example}
	It is impractical to use the definition to directly compute the exterior measure of a given set. However here we calculate the exterior measure of various classes of sets which constitute relatively easy examples.
	\begin{points}
	\item The exterior measure of a point is 0. Indeed, let \( a \in \R^n \) and consider the square of center \( a \) and side \( \epsilon \), \( Q_\epsilon(a) \)\footnote{More precisely, if \( a = (a_1, \cdots, a_n) \) then \( Q_\epsilon(a) = (a_1 - \frac{\epsilon}{2}, a_1 + \frac{\epsilon}{2}) \times \cdots \times (a_1 - \frac{\epsilon}{2}, a_1 + \frac{\epsilon}{2}) \).}. Then \( Q_\epsilon(a) \) is certainly a cover of \( \{ a\} \) and has volume \( \epsilon^n \). That is, \( \ext{\{ a \}} \leq \epsilon^n \). Since \( \epsilon \) can be as small as we wish we conclude \( \ext{\{ a \}} = 0 \).

	\item A segment in \( \R^n \) (with \( n > 1 \)) has exterior measure 0. If the segment has length \( L \) then we can cover it with a rectangle of length \( L \) and whose all other sides have length \( \delta \). Then its total volume is \( L\delta^{n-1} \) and the exterior measure of the segment is bounded by it. And since \( \delta \) can be made as small as we want, we conclude the exterior measure must be 0. The details of the proof are a little cumbersome but the idea is hopefully clear.

	\item In general any (sufficiently well-behaved) bounded subset of a hyperspace of dimension \( k \) inside \( \R^n \) with \( k < n \) has zero exterior measure. The idea is a generalisation of the previous two examples: the set can be covered by an \( n \)-dimensional hypercube in such a way that \( n - k \) of its sides can be shrunk as much as one whishes and so the total volume of the cube goes to zero. Again, this is a little handwavy but the argument can be made precise. 

	\item Any countable set set of \( \R^n \) has zero exterior measure. Since a countable set is a countable union of points, cover one of the points with a square of volume \( \frac{\epsilon}{2} \), the next one with a square of volume \( \frac{\epsilon}{4} \), the following with a square of volume \( \frac{\epsilon}{8} \) and so on. The total volume of the cover is
		\begin{equation*}
			\sum_{n = 1}^{\infty} \frac{\epsilon}{2^n} = \epsilon 
		\end{equation*}
		which can be made as small as one wishes.
	\end{points}
\end{example}

\begin{lemma} \label{lemma:exterior measure with open sets}
	The exterior measure of any set is the same even if only open covers are considered.
\end{lemma}
\begin{proof}
	Let's for the moment write \( M^*(A) \) for the outer measure of a set considering only open covers. It is clear that \( \ext{A} \leq M^*(A) \) since a cover with open rectangles of \( A \) is still a cover by rectangles of \( A \) and so \( M^\ast(A) \) should be at least as big as \( \ext{A} \).

	Now we prove the reverse inequality, \( M^\ast(A) \leq \ext{A} \). If \( \{ R_i \}_{i = 1}^{\infty} \) is a cover of \( A \) by rectangles then
	\begin{equation*}
		\sum_{i = 1}^{\infty} v(R_i) = \sum_{i = 1}^{\infty} v(\mathring{R}_i)  
	\end{equation*}
	since a rectangle and its interior have the same endpoints. In general, however, it is not the case that
	\begin{equation*}
		\bigcup_{i = 1}^{\infty} R_i \subseteq \bigcup_{i = 1}^{\infty} \mathring{R}_i
	\end{equation*}
	since the interior of a set is contained in the set itself and not the other way (as should be the case) and in fact we might not even cover \( A \) anymore.

	To mend this we can simply dilate the interiors. In detail, given a rectangle \( R = \langle a_1, b_1 \rangle \times \cdots \times \langle a_n,b_n \rangle \) to be
	\begin{equation*}
		\lambda R = \lambda \langle a_1, b_1 \rangle \times \cdots \times \lambda \langle a_n,b_n \rangle
	\end{equation*}
	where by definition
	\begin{equation*}
		\lambda \langle a, b \rangle = \left\langle \frac{a+b}{2} - \lambda \frac{b - a}{2}, \frac{a + b}{2} + \lambda\frac{b - a}{2} \right\rangle.
	\end{equation*}
	This is all a very complicated way of saying we slightly inflate the rectangles while keeping their centers the same. It should be clear that if \( \lambda > 1 \) then \( R \subseteq \lambda \mathring{R} \) and \( v(\lambda R) = \lambda^n v(R) \). And so
	\begin{equation*}
		A \subseteq \bigcup_{i = 1}^{\infty} R_i \subseteq \bigcup_{i = 1}^{\infty} \lambda \mathring{R}_i.
	\end{equation*}
	Then, since \( \{ \lambda \mathring{R}_j \}_{j = 1}^{\infty} \) is a cover of \( A \) by open rectangles we have
	\begin{equation*}
		M^*(A) \leq \sum_{j = 1}^{\infty} v(\lambda \mathring{R}_j) = \lambda^n \sum_{j = 1}^{\infty} v(R_j).
	\end{equation*}
	Letting \( \lambda \to 1 \) we obtain that for any cover of \( A \) by rectangles \( \{ R_j \}_{j = 1}^\infty \) then
	\begin{equation*}
		M^*(A) \leq \sum_{j = 1}^{\infty} v(R_j)
	\end{equation*}
	and so \( M^*(A) \leq \ext{A} \) as we wanted.
\end{proof}

\begin{lemma} \label{lemma:exterior measure of rectangle}
	The exterior measure of a rectangle is exactly its volume.
\end{lemma}
\begin{proof}
	Let \( R \) be a rectangle. It is clear that \( \ext{R} \leq v(R) \) since \( R \) covers itself. We need to show then that \( v(R) \leq \ext{R} \). If \( \{ R_j \}_{j = 1}^\infty \) is a cover of \( R \) then we wish to conlcude that
	\begin{equation*}
		v(R) \leq \sum_{j = 1}^{\infty}v(R_j). 
	\end{equation*}

	We can without loss of generality assume that we are dealing with a closed rectangle since \( v(R) = v(\bar{R}) \). And since rectangles are bounded they are compact. Using the previous lemma we need only consider covers by open rectangles and by compactness we can further limit our scope to finite open covers. Now all that is left is to show that if a rectangle is covered by a finite amount of other rectangles then their combined volume is greater than that of the the original rectangle. 

	Let, then, \( R = \langle a_1,b_1 \rangle \times \cdots \times \langle a_n,b_n \rangle \) be a rectangle and \( \{ R_j \}_{j = 1}^N \) be a cover of \( R \) with \( R_j = \langle a^j_1,b^j_1 \rangle \times \cdots \times \langle a^j_n,b^j_n \rangle \). Then we take the projection onto the \( i \)-th dimension and we have that
	\begin{equation*}
		\langle a_i, b_i \rangle \subseteq \bigcup_{j = 1}^N \langle a^j_i, b^j_i \rangle.
	\end{equation*}
	Let \( A_i = \min \{ a^1_i, \cdots, a^N_i \} \) and \( B_i = \max \{ b^1_i, \cdots, b^N_i \} \). Then we have \( \abs{b^j_i - a^j_i} \leq \abs{B_i - A_i} \) and \todo{Finish this proof}

	This last part of the proof consists mainly of technical details. The main insight is that since we need only look at closed rectangles and open covers then we can, by compactness, reduce potentially infinite covers to finite ones and then we can take maximums and minimums without concern.
\end{proof}

{
	\def\currentprefix{prop:properties of exterior measure}	
	\begin{proposition}[Properties of the exterior measure] \label{prop:properties of exterior measure}
		The following are some properties of the exterior measure
		\begin{points}
		\item \locallabel{i} \( \ext{\emptyset} = 0 \).
		\item \locallabel{ii} The exterior measure is increasing, that is if \( A \subseteq B \) then \( \ext{A} \leq \ext{B} \). 
		\item \locallabel{iii} The exterior measure is countably subadditive, i.e.
			\begin{equation*}
				\ext{\bigcup_{j = 1}^\infty A_j} \leq \sum_{j = 1}^{\infty} \ext{A_j}. 
			\end{equation*}
		\item \locallabel{iv} The exterior measure is invariant under translations.
		\item \locallabel{v} If \( A \subseteq \R^n \) and \( \lambda \in \R \) then \( \ext{\lambda A} = \lambda^n \ext{A} \)\footnote{The notation \( \lambda A \) does not refer to a dilation as used in the proof of \cref{lemma:exterior measure with open sets} but rather to the image of \( A \) under scalar multiplication by \( \lambda \) which is more standard.}.
		\item \locallabel{vi} If a set \( A \) satisfies \( \mathring{R} \subseteq A \subseteq \bar{R} \) for a rectangle \( R \) then \( \ext{A} = v(R) \).
		\end{points}
	\end{proposition}
	\begin{proof}
		\localref{i} follows from the fact that any cover is a cover of the empty set. \localref{ii} is because any cover of \( B \) is a cover of \( A \) so \( \ext{A} \) must be less than \( \ext{B} \).

		Proving \localref{iii} requires a bit more work. We may assume that every one of the \( A_j \) has finite exterior measure since otherwise we are dealing with a vacuous statement. Let \( \{ R_i^j \}_{i = 1}^\infty \) be a cover of \( A_j \) such that 
		\begin{equation*}
			\sum_{i = 1}^{\infty} v(R_i^j) \leq \ext{A_j} + \frac{\epsilon}{2^j}. 
		\end{equation*}
		Then
		\begin{equation*}
			\bigcup_{j=1}^\infty A_j \subseteq \bigcup_{j=1}^\infty \bigcup_{i=1}^\infty R_i^j
		\end{equation*}
		so
		\begin{equation*}
			\ext{\bigcup_{j=1}^\infty A_j} \leq \sum_{j=1}^\infty \sum_{i=1}^\infty v(R_i^j) \leq \sum_{j = 1}^{\infty} \ext{A_j} + \frac{\epsilon}{2^j} = \sum_{j = 1}^{\infty} \ext{A_j} + \epsilon.
		\end{equation*}
		And then by letting \( \epsilon \to 0 \) we obtain the countable subadditivity.	

		It should be clear that the volume of a rectangle is invariant under translations. This means that if we have a cover of a set \( A \) then we can transform it, by a translation, into a cover of the translated set \( A + x \) of the same total volume, and viceversa. And so it follows that \( \ext{A} = \ext{A + x} \). This proves \localref{iv}.

		The proof of \localref{v} is very similar. Again, it should be clear that if we scale a rectangle of dimension \( n \) by a factor of \( \lambda \) then its volume picks up a factor of \( \lambda^n \). So, given a cover of \( A \) with total volume \( V \) we can scale it by \( \lambda \) and we obtain a cover of \( \lambda A \) with volume \( \lambda^n V \), and viceversa. Thus we see that \( \ext{\lambda A} = \lambda^n \ext{A} \).

		It follows immediately from \localref{ii} that \( \ext{\mathring{R}} \leq \ext{A} \leq
		\ext{\bar{R}} \). And then, using \cref{lemma:exterior measure of rectangle} we find
		\( \ext{\mathring{R}} = \ext{\bar{R}} = v(R) \) and so \( \ext{A} = v(R) \). 
	\end{proof}
}

\section{Measurable sets}
If the measure we are constructing is to be a useful generalization of the notion of volume we should expect the measure of the union of disjoint sets to be the sum of their measures. With the exterior measure this is the case for most well-behaved sets, but there exist counterexamples. The solution to this problem is to restrict ourselves to a smaller class of sets which we will call the measurable sets.
\begin{definition}[Measurable set]\label{def:measurable set}
	We say a set \( E \subseteq \R^n \) is \emph{measurable} if for any other set \( A \subseteq \R^n \) it is true that
	\begin{equation*}
		\ext{A} \geq \ext{E \cap A} + \ext{E^c \cap A}.
	\end{equation*}
The set \( A \) is sometimes called a \emph{test set}.
\end{definition}

Notice that because the exterior measure is subadditive we get the other inequality for free so we could have required equality in the definition of a measurable set without being more restrictive.

\begin{example} \label{exe:measurable sets}
	The following are various examples of measurable sets
	\begin{points}
	\item \( \R^n \) is measurable since \( \R^n \cap A = A \) and \( (\R^n)^c \cap A = \emptyset \).
	\item Similarly \( \emptyset \) is also measurable.
	\item Any set of zero exterior measure, also called a \emph{null set}, is measurable. Indeed, if \( \ext{E} = 0 \) since \( E \cap A \subseteq E \) then \( \ext{E \cap A} = 0 \) and
		\begin{equation*}
			\ext{E \cap A} + \ext{E^c \cap A} = \ext{E^c \cap A} \leq \ext{A}.
		\end{equation*}
	\end{points}
\end{example}

The collection of measurable subsets of \( \R^n \) forms a \( \sigma \)-algebra. To prove this we will first show a preliminary result. 
\begin{proposition}
	The collection of measurable subsets is stable under finite unions. Furthermore the exterior measure is finitely additive, that is if \( E_1, \cdots, E_n \) are measurable sets and are pairwise disjoint then 
	\begin{equation*}
		\ext{\bigcup_{k = 1}^n E_k} = \sum_{k = 1}^{n} \ext{E_k}. 
	\end{equation*}
\end{proposition}
\begin{proof}
	Let \( \M \) denote the set of measurable subsets of \( \R^n \). It is sufficient to show that if \( E, F \in \M \) then \(  E \cup F \in \M \) since we can then prove by induction that any finite union of measurable sets is measurable. For any \( A \subseteq \R^n \) we have
	\begin{equation*}
		\ext{A \cap (E \cup F)} + \ext{A \cap (E \cup F)^c} = \ext{(A \cap E) \cup (A \cap F)} + \ext{A \cap E^c \cap F^c}.
	\end{equation*}
	We may use the identity of sets \( (A \cap E) \cup (A \cap F) = (A \cap E) \cup (A \cap E^c \cap F) \) and subadditivity to get
	\begin{align*}
		\ext{A \cap (E \cup F)} & + \ext{A \cap (E \cup F)^c} = \\
														& = \ext{(A \cap E) \cup (A \cap E^c \cap F)} + \ext{A \cap E^c \cap F^c} \\
														& \leq \ext{A \cap E} + \ext{(A \cap E^c) \cap F} + \ext{(A \cap E^c) \cap F^c} \\
														& = \ext{A \cap E} + \ext{A \cap E^c} = \ext{A},
	\end{align*}
	where we have used the measurability of \( E \) and \( F \) in the last two steps. 

	We now prove the finite additivity. Let \( E_1, \cdots, E_n \) be pairwise disjoint measurable sets. Then using \( A \cap \left(\bigcup_{k = 1}^n	E_k\right) \) as a test set and \( E_n \) as the measurable set we have, by definition of measurability
	\begin{align*}
		\ext{A \cap \left(\bigcup_{k = 1}^n	E_k\right)} & = \ext{A \cap \left(\bigcup_{k = 1}^n	E_k\right) \cap E_n} + \ext{A \cap \left(\bigcup_{k = 1}^n	E_k\right) \cap E_n^c} \\
																										& = \ext{A \cap E_n} + \ext{A \cap \left(\bigcup_{k = 1}^{n-1}	E_k\right)}.
	\end{align*}
	By induction we find
	\begin{equation*}
		\ext{A \cap \left(\bigcup_{k = 1}^n	E_k\right)} = \sum_{k = 1}^{n} \ext{A \cap E_k} 
	\end{equation*}
	and taking \( A = \bigcup_{k = 1}^n E_k \) we obtain
	\begin{equation*}
		\ext{\bigcup_{k = 1}^n E_k} = \sum_{k = 1}^{n}\ext{E_k} 
	\end{equation*}
	as we wanted.
\end{proof}

\begin{proposition}
	The collection \( \M \) of measurable subsets of \( \R^n \) is a \( \sigma \)-algebra.
\end{proposition}
\begin{proof}
	We have already seen in \cref{exe:measurable sets} that \( \R^n \) and \( \emptyset \) are both measurable. It is also immediate that the complement of a measurable set is also measurable. All that remains to be shown is that \( \M \) is closed under countable unions. 

	Let \( \{ E_k \}_{k = 1}^\infty \) be a countable family of measurable subsets. A first observation is that we may, without loss of generality, assume that the \( E_k \) are pairwise disjoint. Indeed, define \( F_1 = E_1 \) and \( F_k = E_{k} - \bigcup_{j = 1}^{k-1}E_j \). The \( F_k \) are disjoint by construction. They are also all measurable by virtue of being finite intersections and unions of measurable sets and more importantly
	\begin{equation*}
		\bigcup_{k = 1}^\infty E_k = \bigcup_{k = 1}^\infty	F_k.
	\end{equation*}
	What this means is that any union of measurable sets is equal to the union of some other pairwise disjoint measurable sets. So if we manage to show that countable unions of pairwise disjoint measurable sets are measurable we are done. 

	Let's get to it then. We want to show that
	\begin{equation*}
		\ext{A} \geq \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}.
	\end{equation*}
	We start from the fact that finite unions of measurable sets are measurable and so for any \( n \in \N \)
	\begin{equation*}
		\ext{A} \geq \ext{A \cap \left(\bigcup_{k = 1}^n E_k\right)} + \ext{A \cap \left(\bigcup_{k = 1}^n E_k\right)^c}.
	\end{equation*}
	Now notice that 	
	\begin{equation*}
		\bigcap_{k = 1}^N E_k \subseteq \bigcap_{k = 1}^\infty E_k
	\end{equation*}
	so when we take complements the inclusion reverses and we get
	\begin{equation*}
		\left(\bigcap_{k = 1}^\infty E_k\right)^c \subseteq \left(\bigcap_{k = 1}^N E_k\right)^c.
	\end{equation*}
	Therefore
	\begin{equation*}
		\ext{A \cap \left(\bigcup_{k = 1}^n E_k\right)^c} \geq \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}
	\end{equation*}
	which takes care of the second term.

	For the first term, we use the distributivity of intersection over unions and finite additivity ---remember we are assuming the \( E_k \) to be pairwise disjoint--- to get
	\begin{equation*}
		\ext{A \cap \left(\bigcup_{k = 1}^n E_k\right)} = \ext{\bigcup_{k = 1}^n A \cap E_k} = \sum_{k = 1}^{n} \ext{A \cap E_k}.
	\end{equation*}

	All together reads
	\begin{equation*}
		\ext{A} \geq \sum_{k = 1}^{n} \ext{A \cap E_k} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}.
	\end{equation*}
	By taking the limit \( n \to \infty \) we get
	\begin{equation*}
		\ext{A} \geq \sum_{k = 1}^\infty \ext{A \cap E_k} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}.
	\end{equation*}
	Finally, we use subadditivity to arrive at the desired result,
	\begin{align*}
		\ext{A} & \geq \sum_{k = 1}^\infty \ext{A \cap E_k} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c} \\
						& \geq \ext{\bigcup_{k = 1}^\infty A \cap E_k} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c} \\
						& \geq \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}.
	\end{align*}
\end{proof}

We now have a \( \sigma \)-algebra defined on \( \R^n \), which makes into a measurable space. To make it into a measure space we restrict the exterior measure to the measurable sets. We have to check that this is an honest to goodness measure. We have already seen that the measure of the empty set is 0, but we have to show that the measure is additive for disjoint unions.

\begin{proposition}
	The exterior measure of the union of disjoint sets is the sum of their measures. That is, if \( E_k \) are pairwise disjoint measurable sets then
	\begin{equation*}
		\ext{\bigcup_{k = 1}^\infty E_k} = \sum_{k = 1}^{\infty} \ext{E_k}. 
	\end{equation*}
\end{proposition}
\begin{proof}
	We showed in the proof of the previous proposition that
	\begin{equation*}
		\ext{A} \geq \sum_{k = 1}^\infty \ext{A \cap E_k} + \ext{A \cap \left(\bigcup_{k = 1}^\infty E_k\right)^c}.
	\end{equation*}
	Take as a test \( A = \bigcup_{k = 1}^\infty E_k \), then, using the fact that the \( E_k \) are pairwise disjoint,
	\begin{equation*}
		\ext{\bigcup_{k = 1}^\infty E_k} \geq \sum_{k = 1}^\infty \ext{E_k} + \ext{\emptyset} = \sum_{k = 1}^\infty \ext{E_k}.
	\end{equation*}
	The reverse inequality is a statement of subadditivity. So we get
	\begin{equation*}
		\ext{\bigcup_{k = 1}^\infty E_k} = \sum_{k = 1}^\infty \ext{E_k}.
	\end{equation*}
\end{proof}

\section{The structure of measurable sets}
In this section we will give a number of results that shed light into the nature of measurable sets. The \( \sigma \)-algebra \( \M \) is not exactly the Borel \( \sigma \)-algebra \( \B(\R) \). However measaurable sets are really close to Borel sets, as we will see.

{\def\currentprefix{theo:characterization of measurable sets}
\begin{theorem}\label{theo:characterization of measurable sets}
	For any subset \( E \subseteq \R^n \) the following are equivalent:
	\begin{points}
	\item \locallabel{i} \( E \) is measurable.
	\item \locallabel{ii} For all \( \epsilon > 0 \) there is an open set \( G_{\epsilon} \supseteq E \) such that \(  \ext{G_{\epsilon} - E} < \epsilon \).
	\item \locallabel{iii} For all \( \epsilon > 0 \) there is a closed set \( F_{\epsilon} \subseteq E \) such that \(  \ext{E - F_{\epsilon}} < \epsilon \).
	\item \locallabel{iv} For all \( \epsilon > 0 \) there are an open set \( G_{\epsilon} \) and a closed set \( F_{\epsilon} \) such that \( F_{\epsilon} \subseteq E \subseteq G_{\epsilon} \) and \( \ext{G_{\epsilon} - F_{\epsilon}} < \epsilon \).
	\end{points}
\end{theorem}
\begin{proof}
	Let's first show \localref{i}\( \implies \)\localref{ii}. We tackle first the case \(
	\ext{E} < \infty \). Then, by definition of the exterior measure there is a cover of \( E \) by open rectangles, \( \{ R_j \}_{j = 1}^\infty \) such that
	\begin{equation*}
		\sum_{j = 1}^{\infty} v(R_j) < \ext{E} + \epsilon.
	\end{equation*}
	Let \( G_\epsilon = \bigcup_{j = 1}^\infty R_j \). This is an open set that contains \(
	E \). Thus \( G_\epsilon \cup E = E \) and since \( E \) is measurable by hypothesis we
	have
	\begin{equation*}
		\ext{G_\epsilon - E} = \ext{G_\epsilon} - \ext{E} \leq \sum_{j = 1}^{\infty}v(R_j) - \ext{E} < \epsilon.
	\end{equation*}

	There is a usual trick to deal with the case \( \ext{E} = \infty \), which is to write
	it as a countable union of sets of finite measure. Let \( E_N \defeq E
	\cap B(0,N) \). Then every \( E_N \) has finite measure since it is, by construction,
	contained inside of a	ball of finite radius. Furthermore we have
	\begin{equation*}
		E = \bigcup_{N = 1}^{\infty} E_N.
	\end{equation*}
	The inclusion \( \bigcup_{N = 1}^\infty E_N \subseteq E \) is clear, since \( E_N
	\subseteq E \). For the other inclusion we use that if \( x \in E \) then \( x \in E_N
	\) for \( N \geq \norm{x} \).

	Since every \( E_N \) is measurable and has finite measure there exists for each \( N
	\geq 1 \) an open set \( G_N \supseteq E_N \) such that
	\begin{equation*}
		\ext{G_N - E_N} < \frac{\epsilon}{2^N}.
	\end{equation*}
	It should be clear that the \( G_N \) are an open cover of \( E \). Let \( G_\epsilon \)
	be their union, so that \( G_\epsilon \supseteq E \). Now we have
	\begin{equation*}
		G_\epsilon - E = \bigcup_{N = 0}^\infty (G_N - E) \subseteq \bigcup_{N = 0} (G_N -
		E_N)
	\end{equation*}
	thus
	\begin{equation*}
		\ext{G_\epsilon - E} \leq \ext{\bigcup_{N = 1}^\infty(G_N - E_N)} \leq \sum_{N = 1}^{\infty}
		\ext{G_N - E_N} < \sum_{N = 1}^{\infty} \frac{\epsilon}{2^N} = \epsilon  
	\end{equation*}
	as we wanted.	

	We now prove \localref{ii}\( \implies \)\localref{i}.	For each \( n \in \N \) let \( G_n
	\supseteq E \) be an open set such that \( \ext{G_n - E} < \frac{1}{n} \) using our
	hypothesis \localref{ii}. Then let \( N \defeq \bigcap_{N = 1}^\infty G_N - E \). It
	follows \( N \) is a null set, since for each \( n \in \N \) we have
	\begin{equation*}
		\ext{N} = \ext{\bigcap_{n = 1}^\infty G_n - E} \leq \ext{G_n - E} < \frac{1}{n}
	\end{equation*}
	so it must be \( \ext{N} = 0 \). In particular \( N \) is measurable. 

	Now notice that \( N \cup E = \bigcap_{n = 1}^\infty G_n \), thus, since \( N \) and \( E
	\) are disjoint by construction
	\begin{equation}\label{eq:measurable set is intersection of open sets minus null set}
		E = \bigcap_{n = 1}^{\infty} G_n - N.
	\end{equation}
	This means \( E \) is measurable. Indeed, every \( G_n \) is measurable since it is
	open, so their intersection is also measurable. We showed before that \( N \) is
	measurable since it is null, so \( E \) must be measurable since it is a difference of
	measurable sets.

	\parbreak

	Let's show \localref{i}\( \iff \)\localref{iii}. For the implication
	\localref{i}\( \implies \)\localref{iii} apply \localref{ii} to \( E^c \). That is,
	there is an open set \( G_\epsilon \supseteq E^c \) such that \( \ext{G_\epsilon - E^c}
	< \epsilon \). Now \( F_\epsilon \defeq G_\epsilon^c \) is closed and
	\begin{equation*}
		F_\epsilon = G_\epsilon^c \subseteq (E^c)^c = E.
	\end{equation*}
	Furthermore
	\begin{equation*}
		E - F_\epsilon = E \cap F_\epsilon^c = E \cap G_\epsilon = (E^c)^c \cap G_\epsilon =
		G_\epsilon - E^c
	\end{equation*}
	which means \( \ext{E - F_\epsilon} < \epsilon \) as we wanted to show. 

	To prove the converse, \localref{iii}\( \implies \)\localref{i} we use an argument very
	similar to the proof of \localref{ii}\( \implies \)\localref{i}. For every \(
	n \in \N \) let \( F_n \subseteq E \) be a closed set such that \( \ext{E - F_n} <
	\frac{1}{n} \). Then let
	\begin{equation*}
		N \defeq E - \bigcup_{n = 1}^{\infty} F_n.
	\end{equation*}
	It is easy to see that \( N \) is a null set, and therefore measurable. Finally from
	\begin{equation}\label{eq:measurable set is union of closed sets and null set}
		E = N \cup \bigcup_{n = 1}^\infty F_n
	\end{equation}
	one concludes \( E \) is measurable.

	\parbreak

	Finally we will prove that \localref{iv} is equivalent to \localref{ii} and
	\localref{iii}, which in turn means it is equivalent to \localref{iv}.

	It is easy to see that \localref{iv} implies \localref{ii} and \localref{iii}. Indeed, if
	\( G_\epsilon \) and \( F_\epsilon \) satisfy the hypotheses of \localref{iv} then
	\begin{equation*}
		\ext{E - F_\epsilon} \leq \ext{G_\epsilon - F_\epsilon} < \epsilon
	\end{equation*}
	and 
	\begin{equation*}
		\ext{G_\epsilon - E} \leq \ext{G_\epsilon - F_\epsilon} < \epsilon.
	\end{equation*}

	We now prove the converse. If there exist \( G_{\epsilon/2} \) and \(
	F_{\epsilon/2} \) satisfying the conditions in \localref{ii} and \localref{iii} then it is
	easy to show that \( G_{\epsilon/2} - F_{\epsilon/2} = (G_{\epsilon/2} - E) \cup (E -
	F_{\epsilon/2}) \)
	which implies
	\begin{equation*}
		\ext{G_{\epsilon/2} - F_{\epsilon/2}} \leq \ext{G_{\epsilon/2} - E} + \ext{E -
		F_{\epsilon/2}} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
	\end{equation*}
\end{proof}
}

\begin{corollary}
	A set \( E \subseteq \R^n \) is measurable if and only if it is a countable union of open
	sets minus a null set. Equivalently, \( E \) is measurable if and only if it is the
	union of a null set and a countable union of closed sets.
\end{corollary}
\begin{proof}
	It should be clear that a countable union of open sets minus a null set is measurable,
	as is a countable union of closed sets and a null set. The proof of the converse is
	essentially in the proof of \cref{theo:characterization of measurable sets}, namely
	\cref{eq:measurable set is intersection of open sets minus null set,eq:measurable set is
	union of closed sets and null set}.
\end{proof}

\chapter{The Lebesgue integral and integrable functions}
\begin{theorem}[Monotone Convergence] \label{theo:monotone convergence}
	Let \( f_n \colon E \subseteq \R \to [0,\infty] \) an increasing sequence of positive
	measurable functions, i.e. \( f_n \leq f_{n+1} \ae \), which converges pointwise almost everywhere to a measurable  function \( f \). Then
	\begin{equation*}
		\int_E f = \lim_{n \to \infty} \int_E f_n.
	\end{equation*}
\end{theorem}
\begin{proof}
	Observe that since the sequence of the \( f_n \) is increasing then so is the sequence
	of their integrals and therefore it has a limit ---although it is potentially not
	finite---. In particular we have
	\begin{equation*}
		\lim_{n \to \infty} \int_E f_n \leq \int_E f
	\end{equation*}
	since each of the \( f_n \) is bounded by \( f \). Therefore we need only show the
	reverse inequality,
	\begin{equation} \label{eq:integral of limit bounded by limit of integrals}
		\int_E f \leq \lim_{n \to \infty} \int_E f_n
	\end{equation}

	The theorem is easy to prove when the domain of integration has finite measure and the
	convergence of the \( f_n \) to \( f \) is almost everywhere uniform. In this case we
	have that for any \( \epsilon > 0 \) there is a term beyond which
	\begin{equation*}\label{eq:fn uniformly bounded by f}
		f(x) - f_n(x) \leq \epsilon
	\end{equation*}
	holds for almost all \( x \in E \). When we integrate over \( E \), ignoring the null
	set where \cref{eq:fn uniformly bounded by f} does not hold, we get
	\begin{equation*}
		\int_E f_n \geq \int_E f - \int_E \epsilon = \int_E f - \epsilon m(E).
	\end{equation*}
	Letting \( \epsilon \to 0 \) and \( n \to \infty \) we get the result we wanted.

	Let's now tackle the general case. We will show it by way of the definition of the
	Lebesgue integral, i.e. by working with simple functions that are bounded by \( f \).
	Let, then, \( s \) be a positive measurable simple function on \( E \) such that \( s
	\leq f \). We also introduce a parameter \( c \in (0,1) \) to give us some wiggle
	room. Then define
	\begin{equation*}
		E_n = \{ x \in E \mid cs(x) \leq f_n(x) \}.
	\end{equation*}
	Since \( E_n = (cs - f_n)^{-1}([-\infty, 0]) \) it is measurable.	Notice that beyond a
	certain \( n \) the \( E_n \) are not empty. Indeed, for all \(
	x \in E\) we have \( s(x)	\leq f(x) \), thus \( cs(x) < f(x) \) and so, since \(
	f_n(x) \) converges to \(	f(x) \), beyond some \( n \) we must have \( cs(x) < f_n(x) <
	f(x) \) which means \( x \in E_n \). Also, it should be clear	that \( E_n \subseteq
	E_{n+1} \) because of the	monotonicity the \( f_n \). Finally, we have
	\begin{equation*}
		E - N = \bigcup_{n = 1}^\infty E_n
	\end{equation*}
	where \( N \) is a set of measure zero, which reflects the fact that the \( f_n \)
	converge to \( f \) almost everywhere on \( E \).

	Consider a disjoint representation of \( s \), \( s = \sum_{k = 1}^{N} \lambda_k
	\chi_{A_k} \) where the \( A_k \subseteq E \) are pairwise disjoint measurable sets.
	Then we have
	\begin{equation*}
		\int_{E_n} s = \sum_{k = 1}^{N} \lambda_k m(A_k \cap E_n). 
	\end{equation*}
	Then when \( n \to \infty \) since the \( E_n \) increase to \( E-N \),	thus \( A_k \cap
	E_n \) increases to \( A_k - N \). Then, by continuity from below of the Lebesgue
	measure
	\begin{equation*}
		m(A_k \cap E_n) \xrightarrow{n \to \infty} m(A_k - N) = m(A_k)
	\end{equation*}
	Therefore, by the definition of the integral of a simple function,
	\begin{equation} \label{eq:limit of simple integrals}
		\int_{E_n} s = \sum_{k = 1}^{N} \lambda_k m(A_k \cap E_n) \xrightarrow{n \to \infty} \sum_{k = 1}^{N} \lambda_k m(A_k) = \int_E s. 
	\end{equation}

	On \( E_n \) we have \( cs \leq f_n \) which means
	\begin{equation*}
		c \int_{E_n} s \leq \int_{E_n} f_n \leq \int_{E} f_n.
	\end{equation*}
	Now we take the limit \( n \to \infty \) and using \cref{eq:limit of simple integrals}
	we find 
	\begin{equation*}
		c \int_E s \leq \lim_{n \to \infty} \int_E f_n.
	\end{equation*}
	And by letting \( c \to 1 \) we obtain
	\begin{equation*}
		\int_E s \leq \lim_{n \to \infty} \int_E f_n.
	\end{equation*}
	We have just shown that the integral of any simple function bounded by \( f \) is bounded
	by the limit of the integrals \( \int_E f_n \). Thus, the supremum over all simple
	functions bounded by \( f \), which is, by definition the integral of \( f \), is also
	bounded by it,
	\begin{equation*}
		\int_E f \leq \lim_{n \to \infty} \int_E f_n,
	\end{equation*}
	as we wanted.
\end{proof}

The Monotone Convergence Theorem implies a couple of other important results.
\begin{theorem}[Beppo Levi]
	Let \( f_n \to E \subseteq \R \to [0,\infty] \)	be positive measurable functions. Then
	\begin{equation*}
		\int_E \sum_{n = 1}^{\infty} f_n = \sum_{n = 1}^{\infty} \int_E f_n.  
	\end{equation*}
\end{theorem}
\begin{proof}
	Let \( F_N \) denote the \( N \)-th partial sum of the series. Since the \( f_n \) are positive, the \( F_N \) are increasing. We can then apply Monotone Convergence, \cref{theo:monotone convergence}.
\end{proof}
This result shows one of the advantages of the Lebesgue integral in front of the Riemann integral. In the Riemann theory of integration, one required uniform convergence to be able to exchange a series with an integral, whereas in this case pointwise convergence suffices. Notice, however, that the terms must be postive. 

\begin{theorem}[Fatou's Lemma]\label{theo:Fatou}
	Given a sequence of positive measurable functions, \( f_n \colon E \subseteq \R^n \to [0,
	\infty] \) then
	\begin{equation*}
		\int_E \liminf_{n \to \infty} f_n \leq \liminf_{n \to \infty} \int_E f_n.
	\end{equation*}
\end{theorem}
\begin{proof}
	We have, by definition
	\begin{equation*}
		\liminf_{n \to \infty} f_n(x) = \lim_{n \to \infty} \left(\inf_{m \geq f_n}
		f_m(x)\right).
	\end{equation*}
	Observe as well that 
	\begin{equation*}
		\inf_{m \geq n}f_n \leq \inf_{m \geq n+1}f_{m}
	\end{equation*}
	which means we can apply \nameref{theo:monotone convergence} to get
	\begin{equation*}
		\int_E \liminf_{n \to \infty} f_n = \int_{E} \lim_{n \to \infty} \left(\inf_{m \geq n}
		f_m\right) = \lim_{n \to \infty} \int_E \inf_{m \geq n} f_m.
	\end{equation*}
	When we write \( \inf_{m \geq n} f_m \) we mean	that we are taking the infimimum
	pointwise, which means \( \inf_{m \geq n} f_m \) need not coincide with any of the \( f_n
	\). At each point, however, we have for all \( m \geq n \), essentially by definition of
	the infimum,
	\begin{equation*}
		\inf_{m \geq n} f_m(x) \leq f_m(x)
	\end{equation*}
	which means
	\begin{equation*}
		\int_E \inf_{m \geq n} f_m \leq \int_{E} f_m
	\end{equation*}
	thus
	\begin{equation*}
		\int_E \inf_{m \geq n} f_m \inf_{m \geq n} \int_E f_m.
	\end{equation*}
	Finally we put it all together to get the desired bound
	\begin{equation*}
		\int_E \liminf_{n \to \infty} f_n = \lim_{n \to \infty} \int_E \inf_{m \geq n} f_m
		\leq \lim_{n \to \infty} \inf_{m \geq n} \int_E f_m = \liminf_{n \to \infty} \int_E
		f_n.
	\end{equation*}
\end{proof}

\begin{theorem}[Dominated Convergence] \label{theo:dominated convergence}
	Let \( f_n \colon E \subseteq \R^n \to \bar{\R} \) be a sequence of measurable functions which
	converge pointwise to a function \( f \colon E \to \bar{\R} \). If
	the \( f_n \) are almost everywhere dominated by an integrable function \( g \colon E
	\to \bar{\R} \), that is
	\begin{equation*}
		\abs{f_n(x)} \leq g(x) \aeon{E}
	\end{equation*}
	then \( f \) is integrable and
	\begin{equation*}
		\int_E \abs{f - f_n} \xrightarrow{n \to \infty} 0.
	\end{equation*}
	In particular
	\begin{equation*}
		\int_E f_n \xrightarrow{n \to \infty} \int_E f.
	\end{equation*}
\end{theorem}
\begin{proof}
	Observe that the \( f_n \) are integrable: they are measurable by assumption and
	\begin{equation*}
		\int_E \abs{f_n} \leq \int_E g < \infty 
	\end{equation*}
	since \( g \) is integrable. Similarly, \( f \) is also integrable. It is measurable by
	being the limit of measurable functions and since \( \abs{f_n(x)} \leq g(x) \ae \) it
	follows \( \abs{f(x)} \leq g(x) \ae \), which means \( f \) is integrable.

	Let \( h_n = \abs{f_n - f} \). We have
	\begin{equation*}
		h_n(x) = \abs{f_n(x) - f(x)} \leq 2g(x) \aeon{E}.
	\end{equation*}
	Now, \( h_n(x) \) goes to 0 as \( n \to \infty \) everywhere on \( E \) so
	\begin{equation*}
		\int_E 2g = \int_E \lim_{n \to \infty} (2g - h_n).
	\end{equation*}
	Since \( 2g(x) - h_n(x) \geq 0 \ae \) we can apply \nameref{theo:Fatou} ---restricting
	ourselves to \( E \) minus the null set where \( 2g(x) - h_n(x) \geq 0 \) may not
	hold---, to find
	\begin{equation*}
		\int_E 2g = \int_E \lim_{n \to \infty} (2g - h_n) \leq \liminf_{n \to \infty} \int_E
		2g - h_n
	\end{equation*}
	since \( \lim_{n \to \infty} (2g - h_n) = \liminf_{n \to \infty }(2g - h_n) \). Now,
	using standard properties of the limit inferior,
	\begin{align*}
		\int_E 2g & \leq \liminf_{n \to \infty} \int_E 2g - h_n = \liminf_{n \to \infty}
		\left(\int_E 2g - \int_E h_n\right) \\
							& = \int_E 2g + \liminf_{n \to \infty} \left(- \int_E h_n\right) \\
							& = \int_E 2g - \limsup_{n \to \infty} \int_E
							h_n.
	\end{align*}
	From this it follows that \( \limsup_{n \to \infty} \int_E h_n \leq 0 \), which means it
	is actually 0 since the \( h_n \) are positive. In fact, since they are positive we have
	that \( \liminf_{n \to \infty} \int_E h_n \geq 0 \). Since the limit superior is always
	greater than the limit inferior, we conclude both are 0 for \( \int_E h_n \) which means
	\begin{equation*}
		\int_E h_n = \int_E \abs{f_n - f} \xrightarrow{n \to \infty} 0,
	\end{equation*}
	as we wanted.	

	Finally we have
	\begin{equation*}
		\abs{\int_E f_n  - \int_E f} = \abs{\int_E f_n - f} \leq \int_E \abs{f_n - f} \to 0
	\end{equation*}
	which implies
	\begin{equation*}
		\int_E f_n \to \int_E f
	\end{equation*}
	as we wished.
\end{proof}

\section{Differentiation under the integral sign}

\begin{theorem}
	Let \( f \colon E \times I \to \R \) be a measurable function where \( E \subseteq \R^n
	\) is measurable and \( I	\subseteq \R \) is an interval. Write \( f_x \) for the
	function
	
	
	
\end{theorem}


\section{Applications of the theory of integration}
Once we have the various results of the theory of Lebesgue integration at out disposal we
can restate a number of concepts in terms of the Lebesgue integral. These include the
convolution of functions, differentiation under the integral sign and the change of
variable theorem. 

\subsection{Convolution}
The operation of convolution is defined on integrable functions. 

\begin{definition}[Convolution]
	Let \( f, g \in \L^1(\R^n) \) be two integrable functions. We define their
	\emph{convolution} \( f \ast g \) as
	\begin{equation*}
		(f \ast g)(x) \defeq \int_{\R^n} f(x - y) g(y) \d y.
	\end{equation*}
\end{definition}
If the functions \( f \) and \( g \) are continous and have compact support it is clear
that their convolution is finite everywhere. But in fact we can show that the convolution
of any two integrable functions is always integrable.

\begin{proposition}
	The convolution of any two integrable funcions is always integrable. In other words, if
	\( f, g \in \L^1(\R^n) \) then \( f \ast g \in \L^1(\R^n) \).
\end{proposition}
\begin{proof}
	To show that \( f \ast g \) is integrable we need to show that it is measurable and
	absolutely integrable. We can directly compute the integral of \(
	\abs{f \ast g }	\):
	\begin{align*}
		\int_{\R^n} \abs{f \ast g} & = \int_{\R^n} \int_{\R^n} \abs{f(x - y)} \abs{g(y)} \d y \d
		x \\
															 & = \int_{\R^n} \abs{g(y)} \int_{\R^n} \abs{f(x - y)} \d x
															 \d y \\
															 & = \int_{\R^n} \abs{g(y)} \d y \int_{\R^n} \abs{f(x - y)}
															 \d x \tag*{by Tonelli's Theorem} \\
															 & = \int_{\R^n} \abs{g} \int_{\R^n} \abs{f} < \infty
															 \tag*{since \( f,g \in \L^1(\R^n). \)}
	\end{align*}

	In the last step we made use of the change of variable theorem which we haven't shown yet.
	However it should not be difficult to convince yourself that it is true in this case since
	\( f(x - y) \) is merely \( f \) translated by \( y \), so its integral over \( \R^n \)
	should not change. 

	It is clear that \( h(x,y) = f(x - y)g(y) \) is measurable since it is the product of
	measuarable functions \todo{Not as clear}. We have just shown that it is absolutely integrable, so, by
	Fubini's Theorem we find that the integral of \( h \) with respect to \( y \), that is,
	\( f \ast g \) is measurable. From this we conclude that \( f \ast g \in \L^1(\R^n) \).
\end{proof}

The operation of convolution has a number of nice properties, namely it is associative and
commutative.
\begin{proposition}
	Convolution is an associative and commutative operation. That is, for any \( f, g, h \in
	\L^1(\R^n) \) we have
	\begin{equation*}
		(f \ast g) \ast h
	\end{equation*}
	and
	\begin{equation*}
		f \ast g = g \ast f.
	\end{equation*}
\end{proposition}
\begin{proof}
	We show commutativity first.
	\begin{align*}
		(f \ast g)(x) & = \int_{\R^n} f(x - y)g(y) \d y \\
									& = \int_{\R^n} f(z)g(x - z) \d z	\tag*{using the change \( z = x -y
									\)} \\
									& = \int_{\R^n} g(x - z)f(z) \d z = (g \ast f)(x).
	\end{align*}

	\begin{align*}
		((f \ast g) \ast h)(x) & = \int_{\R^n} (f \ast g)(x - y) h(y) \d y \\
													 & = \int_{\R^n} \left(\int_{\R^n} f(x - z)g(z) \d z\right) h(y) \d y
													 & = 
	\end{align*}

\end{proof}

\part{Banach Spaces}
\chapter{Definition and Examples}
The theory of Banach and Hilbert spaces generalises the basic analytical and geometric
ideas and tools of Euclidean space to a more general class of spaces. By Euclidean space
we mean \( \R^n \) with the topology that is induced by the standard inner product, i.e.
the topology generated by open balls. Our starting point will be normed vector spaces.
From now on \( E \) will denote a vector space over either the real or the complex
numbers. If the distinction is meaningful at any point we will make note of it. We will
refer to elements of the base field simply as scalars, unless whether they are real or
complex is relevant.

\section{Normed spaces}
\begin{definition}[Norm]
	A norm on a vector space \( E \) is a function \( \norm{\cdot} \colon E \to [0, \infty)
	\) such that
	\begin{points}
	\item \( \norm{0} \) if and only if \( x = 0 \),
	\item \( \norm{x + y} \leq \norm{x} + \norm{y} \), which is known as the \emph{triangle
		inequality},
	\item for any scalar \( \lambda \), \( \norm{\lambda x} = \abs{\lambda}\norm{x} \).
	\end{points}
\end{definition}

A vector space equipped with a norm is called a \emph{normed space}. A norm can be used to
define a \emph{metric} or \emph{distance}, which is the function
\begin{align*}
	d \colon E \times E & \longrightarrow [0,\infty) \\
	(x,y) & \longmapsto \norm{x - y}.
\end{align*}
It is easy to check that \( d \) indeed satisfies the definition of a metric:
\begin{points}
\item By definition \( d(x,y) = 0 \) if and only if \( \norm{x - y} = 0 \), which is
	equivalent to \( x - y = 0 \) and thus \( x = y \).
\item \( d(x,y) = \norm{x - y} = \abs{-1}\norm{y - x} = \norm{y - x} = d(y,x) \).
\item \( d(x,y) = \norm{x - z} = \norm{x - y + y - z} \leq \norm{x - y} + \norm{y - z} =
	d(x,y) + d(y,z) \).
\end{points}

We have that a normed space is also a metric space and therefore a topological space, with
the topology generated by the open balls
\begin{equation*}
	B(x,r) \defeq \set{y \in E \mid \norm{x - y} < r}.
\end{equation*}
Thus, a subset \( U \subseteq E \) is open if and only if for every \( x \in E \) there
exists \( r > 0 \) such that \( B(x,r) \subseteq U \).

Note that while a norm always induces a metric, it is not true that every metric comes
from a norm.

There is further structure we could give to our vector space, namely a \emph{scalar
product}. This is a positive definite symmetric bilinear form \( \inn{\cdot}{\cdot} \colon
E \times E \to \R \). This requires that \( E \) be a real vector space. The analog for
complex spaces is a \emph{Hermitian product}, which is a positive definite, conjugate
symmetric bilinear form \( \inn{\cdot}{\cdot} \colon E \times E \to \C \). We will deal
with these later on when discussing Hilbert spaces. For now note that a scalar product
infuces a norm by
\begin{equation*}
	\norm{x} \defeq \sqrt{\inn{x}{x}}.
\end{equation*}

\section{Convergence and completeness}
Since normed spaces are metric spaces we can speak of convergence. 
\begin{definition}[Convergence]
	We say a sequence \( (x_n) \) in a normed space \( E \) \emph{converges} to \( x \in E \) if
	for every \( \epsilon > 0 \) ther exists \( N \in \N \) such that when \( n \geq N \)
	one has
	\begin{equation*}
		\norm{x_n - x} < \epsilon.
	\end{equation*}
\end{definition}
The intuition behind this definition is exactly the same as in the case of Euclidean
space: the terms of the sequence get arbitrarily close to a certain point \( x \) which is
naturally called the \emph{limit} of \( x_n \). We will write \( \lim_{n \to \infty} x_n =
x\) or \( x_n \xrightarrow{n \to \infty} x \) when \( (x_n) \) converges to \( x \).

There is a long list of facts about limits and convergence that are shown for Euclidean
spaces in first and second year Analysis which are also true for any normed space, namely
those that only make use of the normed vector space structure of Euclidean space. Because
of this, their proofs can be repeated verbatim for any normed space. For example, the
limit of a sum is the sum of limits, scalars can move in and out of limits...

An important class of sequences are Cauchy sequences:
\begin{definition}[Cauchy sequence]
	A sequence \( (x_n) \) is a \emph{Cauchy sequence} if for any \( \epsilon > 0 \) there
	exists \( N \in \N \) such that for all \( n, m > N \) one has
	\begin{equation*}
		\norm{x_n - x_m} < \epsilon.
	\end{equation*}
\end{definition}
It is easy to show that a convergent sequence is a Cauchy sequence. Indeed, if \( x_n \to
x\) then for any \( \epsilon > 0 \) there is \( N \in \N \) such that if \( n, m > N \)
\begin{equation*}
	\norm{x_n - x_m} \leq \norm{x_n - x} + \norm{x_m - x} < 2\epsilon.
\end{equation*}
The converse if, of course, not true in general. The most famous offender is the
set of real numbers. If there exists a Cauchy sequence that fails to be convergent it
means that our space has a sort of hole. Indeed, the terms of a Cauchy sequence all get
arbitrarily close to each other so if they fail to converge it means they are circling a
point which should be there but isn't.  

A useful fact to keep in mind is that the norms of the terms of a Cauchy sequence are
convergent.
\begin{proposition}\label{prop:norms of Cauchy sequence are convergent}
	If \( x_n \) is a Cauchy sequence in a normed space \( E \), then the sequence \(
	\norm{x_n}  \) is convergent in \( \R \).
\end{proposition}
\begin{proof}
	This follows from the Cauchy condition and the reverse triangle inequality. Indeed, for
	sufficiently large \( n \) and \( m \) we have
	\begin{equation*}
		\norm{x_n - x_m} < \epsilon.
	\end{equation*}
	Then
	\begin{equation*}
		\abs{\norm{x_n} - \norm{x_m}} \leq \norm{x_n - x_m} < \epsilon
	\end{equation*}
	thus the real sequence \( \norm{x_n} \) is Cauchy and thus convergent. 
\end{proof}

A metric space in which every Cauchy sequence is convergent is called \emph{complete}, and
a complete normed space is known as \emph{Banach space}.

\section{Examples}
The examples of Banach spaces come in various different flavours. In this section we will
discuss three of these: spaces of functions on a compact domain, sequence spaces and
Lebesgue spaces.
\subsection{Function spaces}
Consider a compact set, \( K \subseteq \R^m \). Then we can consider various sets of
functions defined on \( K \). We will write, as is standard, \( B(K) \) for the set of all
bounded functions on \( K \), \( \cont{K} \) for the set of all continuous functions on \(
K \) and \( \class{n}{K} \) for the set of \( n \) times differentiable functions with
continuous \( n \)-th partial	derivatives on \( K \). All of these sets are actually
vector spaces with pointwise addition and scalar multiplication. And in fact we have the
chain of inclusions
\begin{equation*}
	B(K) \supseteq \cont{K} \supseteq \class{1}{K} \supseteq \cdots \supseteq \class{n}{K}
	\supseteq \cdots.
\end{equation*}
All of these are standard results from elementary real analysis. On \( B(K) \) we can
define the norm
\begin{equation*}
	\norm{f}_\infty \defeq \sup_{x \in K} \abs{f(x)}.
\end{equation*}
Which we will refer to as the \emph{uniform norm}. This is indeed a norm ---check it!---, which
makes \( B(K) \) into a normed space.  Restricting it to each of the \( \class{n}{K} \)
also makes them into normed spaces.  Convergence with respect to this norm is known as
\emph{uniform convergence}.

% TODO: Provide picture to give intuition for the balls in this norm

With the uniform norm, however, only	\( B(K) \) and \( C^0(K) \) are complete, therefore Banach
spaces. Let's show it. Let \( (f_n) \) be a Cauchy sequence of bounded functions on \( K
\), i.e. for all \( \epsilon > 0 \) there is \( N \in \N \) such that if \( n, m > N \)
then
\begin{equation}\label{eq:uniformly cauchy}
	\norm{f_n - f_m} = \sup_{x \in K} \abs{f_n(x) - f_m(x)} < \epsilon.
\end{equation}
One often says that the sequence is \emph{uniformly Cauchy}, as opposed to pointwise
Cauchy, which would mean that the sequence \( f_n(x) \) is Cauchy at every point \( x \in
K\). \Cref{eq:uniformly cauchy} implies that the sequence is pointwise Cauchy. Indeed, at
any \( x \in K \) we have
\begin{equation*}
	\abs{f_n(x) - f_m(x)} < \norm{f_n - f_m}_\infty < \epsilon
\end{equation*}
for sufficiently large \( n \) and \( m \). In particular, this means the sequence of real
numbers \( (f_n(x)) \) is convergent since it is Cauchy. This gives us a candidate for the
limit of the \( f_n \) by defining
\begin{align*}
	f \colon K & \longrightarrow \R \\
	x & \longmapsto \lim_{n \to \infty} f_n(x).\footnote{There really is no magic here,
		since it is easy to show that if a sequence is uniformly convergent then it is also
	pointwise convergent and both limits must coincide.}
\end{align*}

If \( f \) is indeed the uniform limit of the \( f_n \) then we must show that \( f \) is
bounded and that \( \norm{f - f_n}_\infty \xrightarrow{n \to \infty} \infty \). Let's
first show that \( f \) is bounded, so \( f \in B(K) \). Using \Cref{prop:norms of Cauchy
sequence are convergent} we find that the sequence \( \norm{f_n}_\infty \) is convergent. It then follows that for all \( x \in K \)
\begin{equation*}
	\abs{f(x)} = \lim_{n \to \infty} \abs{f_n(x)} \leq \lim_{n \to \infty}
	\norm{f_n}_\infty
\end{equation*}
which means \( f \) is bounded, as we wanted.

Let's now show that the convergence is uniform. At every \( x \in K \) we have, for
sufficiently large \( n \) and \( m \)
\begin{equation*}
	\abs{f_n(x) - f_m(x)} < \epsilon
\end{equation*}
which means that when we take the limit \( m \to \infty \) we obtain
\begin{equation*}
	\abs{f_n(x) - f(x)} \leq \epsilon
\end{equation*}
thus \( \norm{f_n - f} \leq \epsilon \) for a sufficiently large \( n \), which shows the
convergence is indeed uniform.

This shows \( B(K) \) is complete. To prove that \( \cont{K} \) is complete we must show
that if the \( f_n \) are contiuous then their uniform limit is also continuous, not just
bounded. This is fairly straightforward once we know the convergence of the \( f_n \) is
uniform. There is \( N \in \N \) beyond which \( \norm{f - f_n}_\infty < \epsilon \), and
since the \( f_n \) are continuous, if \( x \) and \( y \) are sufficiently close then \(
\abs{f_n(x) - f_n(y)} < \epsilon \). Putting this all together we find
\begin{align*}
	\abs{f(x) - f(y)} & \leq \abs{f(x) - f_n(x)} + \abs{f_n(x) - f_n(y)} + \abs{f_n(y) -
	f(y)} \\
										& \leq \norm{f - f_n}_\epsilon + \abs{f_n(x) - f_n(y)} + \norm{f_n -
										f}_\infty < 3\epsilon
\end{align*}
so \( f \) is also continuous.

As we mentioned before, \( \class{n}{K} \) is not complete with respect to the uniform
norm for \( n \geq 1 \). For now let's take \( K \) to be a closed interval, say \( [a,b]
\). There exist sequences of differentiable functions whose limit is not differentiable,
and furthermore, there are sequences of differentiable functions whose limit is also
differentiable but its derivative need not be the limit of the derivatives of the
functions of the sequence. This then shows that \( \class{1}{[a,b]} \). To ensure that
derivatives interact well with limits one needs to require not only uniform convergence of
the functions but also uniform convergence of the derivatives, which then ensures that the
derivative of the limit is the limit of the derivatives. This sort of convergence is
captured by the so-called \( C^1 \) norm
\begin{equation*}
	\norm{f}_{C^1} \defeq \norm{f}_\infty + \norm{f'}_\infty
\end{equation*}
which then makes \( \class{1}{[a,b]} \) into a Banach space. More generally, the \( C^n \)
norm
\begin{equation*}
	\norm{f}_{C^n} \defeq \sum_{k = 0}^{n} \norm{f^{(k)}}_\infty 
\end{equation*}
makes \( \class{n}{[a,b]} \) into a normed space. The appropriate generalisation to an \(
m \)-dimensional fomain must take into account all possible partial derivatives.

\subsection{Sequence Spaces}
Another common family of Banach spaces are the so-called sequence spaces. They are all
subsets of the space of real\footnote{All of these examples also work if one replaces real
with complex} sequences, \( \R^\N \). This is a vector space with pointwise addition and
scalar multiplication. There are various subspaces of \( \R^\N \) which can be made into
Banach spaces with the appropriate norm. First, we have the space \( \ell^\infty \) of
bounded sequences with the uniform norm defined as
\begin{equation*}
	\norm{x}_\infty \defeq \sup_{n \in \N} \abs{x(n)}. \footnote{Since we will be dealing with
		sequences of sequences we adopt the convention to write \( x(n) \) for the \( n \)-th
		term of the sequence \( x \), as opposed to the more standard \( x_n \), and reserve the
	subscript to refer to the terms of a sequence in \( \R^\N \).}
\end{equation*}
The proof that this is indeed a Banach space is essentially identical to the proof that \(
B(K) \) is a Banach space, so I will spare you the details. The main steps are to show
that, given a Cauchy sequence \( x_n \) in \( \ell^\infty \), the sequence of images \(
x_n(m) \) is Cauchy, which gives a pointwise limit, which is then shown to be bounded and
finally one shows that the sequence \( x_n \) converges to its pointwise limit uniformly. 

For \( p > 1 \) one talks about the \( \ell^p \) spaces, which are the spaces of
sequences \( x \) such that
\begin{equation*}
	\sum_{n = 1}^{\infty} \abs{x(n)}^p < \infty 
\end{equation*}
each with the so-called \( p \)-norm
\begin{equation*}
	\norm{x}_p \defeq \left(\sum_{n = 1}^{\infty} \abs{x(n)}\right)^{\frac{1}{p}}.
\end{equation*}
Showing that the \( p \)-norms satisfy the triangle inequality is not trivial ---at least
not for \( p > 1 \)---. We will see that in fact they do when we prove Minkowski's
inequality. 

For the sake of illustration, I will show the completeness of \( \ell^1 \). This will be
standard fare. To begin, we have a Cauchy sequence in	\( \ell^1 \), \( x_n \). The Cauchy condition
with respect to the 1-norm means that for sufficiently large \( n \) and \( m \) one has
\begin{equation*}
	\norm{x_n - x_m}_1 = \sum_{k = 1}^{\infty} \abs{x_n(k) - x_m(k)} < \epsilon. 
\end{equation*}
Since all of the terms of the series are postive this means, in particular, that for any
\( k \in \N \)
\begin{equation*}
	\abs{x_n(k) - x_m(k)} < \norm{x_n - x_m} < \epsilon
\end{equation*}
meaning the sequence \( x_n(k) \) is Cauchy for any \( k \) and therefore convergent. We
then define the pointwise limit of the \( x_n \) as 
\begin{equation*}
	x(k) \defeq \lim_{n \to \infty} x_n(k).
\end{equation*}
We need to show that \( x \in \ell^1 \) and that \( x_n \to x \) uniformly.

To show that \( x \in \ell^1 \) we will make use of \Cref{prop:norms of Cauchy sequence
are convergent}, which gives us that the sequence \( \norm{x_n}_1 \) is convergent and in
particular bounded. That is, there is \( M \in \R \) such that \( \norm{x_n}_1 \leq M \)
for all \( n \in \N \). Let's write this out: for all \( n \in \N \) we have
\begin{equation} \label{eq:partial sums are bounded}
	\sum_{k = 1}^{\infty} \abs{x_n(k)} < M. \tag{\( \ast \)} 
\end{equation}
Because every term in the series is positive we can cut it off wherever we want, meaning
for any \( K \in \N \) we have the bound
\begin{equation*}
	\sum_{k = 1}^{K} \abs{x_n(k)} < M. 
\end{equation*}
Now, by definition, the norm of \( x \) is
\begin{equation*}
	\norm{x}_1 = \sum_{k = 1}^{\infty} \abs{x(k)} = \sum_{k = 1}^{\infty} \abs{\lim_{n \to
	\infty} x_n(k)} = \sum_{k = 1}^{\infty} \lim_{n \to \infty} \abs{x_n(k)}.
\end{equation*}
Because all of the terms of this series are positive, to show it converges it suffices to
show that its partial sums are bounded, but this follows immediately from the bound in
\eqref{eq:partial sums are bounded}:
\begin{equation*}
	\sum_{k = 1}^{K} \lim_{n \to \infty} \abs{x_n(k)} = \lim_{n \to \infty} \sum_{k =
	1}^{K} \abs{x_n(k)} < M.
\end{equation*}
Thus \( \norm{x}_1 < \infty \) and \( x \in \ell^1 \).

\section{Inequalities}

\begin{definition}[Conjugate exponents]
	We say two numbers \( p, q \in (1, \infty) \) are \emph{conjugate exponents} if
	\begin{equation*}
		\frac{1}{p} + \frac{1}{q} = 1.
	\end{equation*}
	By convention, the conjugate exponent of 1 is taken to be \( \infty \), and vice versa.
\end{definition}
Note that 2 is its own conjugate exponent.

\begin{theorem}[Young's Inequality]\label{theo:young}
	If \( p \) and \( q \) are conjugate exponents then the inequality
	\begin{equation*}
		ab \leq \frac{a^p}{p} + \frac{b^q}{q}
	\end{equation*}
	holds for any two \( a, b \in \R \).
\end{theorem}
\begin{proof}
	This follows from the fact that the logarithm is a concave function, for
	\begin{equation*}
		\log\left(\frac{a^p}{p} + \frac{b^q}{q}\right) \geq \frac{1}{p} \log\left(a^p\right) +
		\frac{1}{q}\log\left(b^q\right) = \log(a) + \log(b) = \log(ab).
	\end{equation*}
	And since the logarithm is increasing,
	\begin{equation*}
		ab \leq \frac{a^p}{p} + \frac{b^q}{q}
	\end{equation*}
	as we wanted.
\end{proof}

\begin{theorem}[Hölder's Inequality]\label{theo:holder}
	For any two measurable functions, \( f \) and \( g \), on a measurable domain \( E
	\subseteq \R \), and conjugate exponents \( p, q \in (1,\infty) \) one has the
	inequality
	\begin{equation*}
		\int_E \abs{fg} \leq \left(\int_E \abs{f}^p\right)^{\frac{1}{p}} \left(\int_E \abs{g}^q\right)^{\frac{1}{q}}
	\end{equation*}
\end{theorem}
\begin{proof}
	If \( \int_E \abs{f}^p \) is infinite then the inequality is trivially true, as it is if
	\( \int_E \abs{g}^q \) is infinite. On the other hand, if either of the integrals on the
	right is zero then either \( f \) or \( g \) is zero almost everywhere, which means the
	integral on the left is zero and the inequality is also true.  We can then asssume that
	both integrals are finite and strictly positive. 
	
	Let
	\begin{equation*}
		\tilde{f} \defeq \frac{f}{\left(\int_E \abs{f}^p\right)^\frac{1}{p}}
	\end{equation*}
	and
	\begin{equation*}
		\tilde{g} \defeq \frac{g}{\left(\int_E \abs{g}^q\right)^{\frac{1}{q}}}.
	\end{equation*}
	It is clear that both \( \abs{\tilde{f}}^p \) and \( \abs{\tilde{g}}^q \) integrate to 1
	over \( E \). Using \nameref{theo:young} we have
	\begin{equation*}
		\abs{\tilde{f}} \abs{\tilde{g}} \leq \frac{1}{p} \abs{\tilde{f}}^p + \frac{1}{q}
		\abs{\tilde{g}}^q.
	\end{equation*}
	From this follows, integrating over \( E \),
	\begin{equation*}
		\int_E \abs{\tilde{f} \tilde{g}} \leq \frac{1}{p} \int_E \abs{\tilde{f}}^p +
		\frac{1}{q} \int_E \abs{\tilde{g}}^q \leq \frac{1}{p} + \frac{1}{q} = 1.
	\end{equation*}
	And this implies
	\begin{equation*}
		\int_E \abs{fg} \leq \left(\int_E \abs{f}^p\right)^\frac{1}{p} \left(\int_E
		\abs{g}^q\right)^\frac{1}{q}
	\end{equation*}


	
	
\end{proof}
\begin{corollary}[Hölder's Inequality for \( L^p \) spaces]
	Given \( f \in L^p(E) \) and \( g \in L^q(E) \) where \( p \) and \( q \) are conjugate
	exponents we have that \( fg \in L^1(E) \) and
	\begin{equation*}
		\norm{fg}_1 \leq \norm{f}_p \norm{g}_q.
	\end{equation*}
\end{corollary}	
\begin{proof}
	The case where \( p, q \in (1,\infty) \) is \nameref{theo:holder}. And for the case
	where \( p = 1 \) and \( q = \infty \) we have
	\begin{equation*}
		\norm{fg}_1 = \int_E \abs{fg} \leq \int_E \abs{f} \norm{g}_\infty = \norm{g}_\infty
		\int_E \abs{f} = \norm{f}_1 \norm{g}_\infty.
	\end{equation*}
\end{proof}

\chapter{Bounded Operators}
\section{Bounded Operators}
We now ask what the appropriate structure preserving maps are between Banach spaces and
more generally normed spaces. They should certainly be linear, since normed spaces are
vector spaces. They also are, of course, normed, so the maps we will be interested in
should somehow preserve that. We could simply ask that they be continuous or we could be
stricter and ask that they preserve the norm. Or we could settle somewhere in-between and
ask that they be nondecreasing in norm, meaning \( \norm{Tx} \leq \norm{x} \). However,
the agreed upon notion in the field is that of boundedness.

\begin{definition}[Bounded operator]
	Let \( E \) and \( F \) be normed spaces. We say a map \( T \colon E \to F \) is
	\emph{bounded} if there exists \( M > 0 \), sometimes called a bounding constant, such
	that for all \( x \in E \)
	\begin{equation*}
		\norm{Tx} \leq M \norm{x}\footnote{Strictly speaking, since \( E \) and \( F \) both
			come with their own norms we should somehow distinguish between them. In most cases,
			however, it is clear which norm we are talking about from the context ---for
			instance, \( \norm{Tx} \) must refer to the norm of \( F \) since \( Tx \in F \)---
			so we will mostly dispense with any distinction unless required.}.
	\end{equation*}
	
	We call a bounded linear map a \emph{bounded operator}.
\end{definition}
When dealing with linear maps in the context of normed spaces it is customary to ommit
parentheses when writing the image of an element ny a map, that is, we will generally
write \( Tx \) instead of \( T(x) \).

\parbreak

As it turns out, linear maps are bounded if and only if they are continuous, so really we
are settling for the weaker class of structure preserving maps: continuous and linear. 
{\def\currentprefix{prop:characterisation of bounded operators}
\begin{proposition}
	Let \( E \) and \( F \) be normed spaces and \( T \colon E \to F \) a linear map between
	the two. Then the following are all equivalent:
	\begin{points}
	\item \locallabel{i} \( T \) is bounded.
	\item \locallabel{ii} \( T \) is continuous. 
	\item \locallabel{iii} \( T \) is continuous at 0.
	\item \locallabel{iv} The image of the unit ball of \( E \) by \( T \), \( T(B(0,1)) \) is bounded. 
	\item \locallabel{v} \( T \) sends bounded sets to bounded sets.
	\end{points}
\end{proposition}
\begin{proof}
	Let's first show \localref{ii}\( \iff \)\localref{iii}. Obviously, if \( T \) is
	continuous then it is continuous at 0. Suppose, then, \( T \) is continuous at 0. Let \(
	x_n \) be any sequence in	\( E \) that converges to \( x \). Then \( x_n - x \) converges
	to 0. Because \( T \) is continuous at 0, \( T(x_n - x) \to T(0) \). Because \( T \) is
	linear, \( T(0) = 0 \), and again because of linearity, \( T(x_n - x) = T(x_n) - T(x)
	\). Thus \( T(x_n) \) converges to \( T(x) \), which implies \( T \) is continuous at \(
	x \in E \), therefore continuous. 

	Now we prove \localref{iii}\( \implies \)\localref{i}. \( T \) being continuous at 0
	means that for any \( \epsilon > 0 \) there is \( \delta > 0 \) such that if \( \norm{x}
	< \delta \) then \( \norm{Tx} < \epsilon \). Choose \( \epsilon = 1 \). Then, for any \(
	x \in E \) we have
	\begin{equation*}
		\norm{\frac{\delta}{2} \frac{x}{\norm{x}}} = \frac{\delta}{2} < \delta
	\end{equation*}
	thus
	\begin{equation*}
		\norm{\frac{\delta}{2}\frac{Tx}{\norm{x}}} < 1
	\end{equation*}
	which means, for any \( x \in E \) we have
	\begin{equation*}
		\norm{Tx} < \frac{2}{\delta} \norm{x}
	\end{equation*}
	which shows \( T \) is bounded.

	Conversely, if \( T \) is bounded it is easy to show it is continuous at 0. If \( M \) is
	a bounding constant of \( T \) then, for any \( \epsilon > 0 \), provided \( \norm{x} <
	\frac{\epsilon}{M} \) we have
	\begin{equation*}
		\norm{Tx} < M\norm{x} < \epsilon
	\end{equation*}
	meaning \( T \) is continuous at 0 ---since \( T(0) = 0 \)---.

	We now show \localref{i}\( \iff \)\localref{iv}. If \( T \) is bounded with bounding
	constant \( M \), then, for any \( x \in B(0,1) \) we have
	\begin{equation*}
		\norm{Tx} \leq M \norm{x} < M
	\end{equation*}
	which means \( T(B(0,1)) \subseteq B(0,M) \), which shows \( T(B(0,1)) \) is bounded.
	Conversely, if \( T(B(0,1)) \subseteq B(0,M) \) then for any \( x \in E \) we have
	\begin{equation*}
		\frac{\norm{Tx}}{\norm{x}} = \norm{T\frac{x}{\norm{x}}} < M
	\end{equation*}
	therefore \( \norm{Tx} < M\norm{x} \), showing \( T \) is bounded.

	Finally we prove \localref{iv}\( \iff \)\localref{v}. \localref{v}\( \implies
	\)\localref{iv} is obvious, since the unit ball is by definition bounded. On the other
	hand, if \( A \subseteq E \) is bounded then there is \( R > 0 \) such that \( A
	\subseteq B(0,R) = R(B(0,1)) \), thus
	\begin{equation*}
		T(A) \subseteq T(B(0,R)) = RT(B(0,1))
	\end{equation*}
	which is bounded, provided \( T(B(0,1)) \) is bounded.
\end{proof}
}

\section{Compactness}
\begin{theorem}
	A metric space \( X \) is compact if and only if every sequence in \( X \) has a
	convergent subsequence.\footnote{This property is usually called \emph{sequential
	compactness}.}
\end{theorem}
\begin{proof}
	\( (\implies) \) We will show the contrapositive: if \( X \) has a sequence with no
	convergent subsequence then it cannot be compact. Let \( (x_n) \) be such a sequence.
	This means, in particular, that the sequence must have infinitely many different terms
	---i.e., its image cannot be finite--- since otherwise we would be able to find a convergent
	subsequence.	Since no subsequence of	\( (x_n) \) is convergent, for any point \( y \in
	X \) there must be an open ball \( B(y, r_y) \) which only contains finitely many
	different terms of the sequence ---otherwise \( y \) would be an accumulation point of
	the sequence and therefore the  limit of a subsequence---. Thus we have an open cover of
	\( X \),
	\begin{equation*}
		X = \bigcup_{y \in X} B(y, r_y).
	\end{equation*}
	If we were able to extract a finite subcover from this cover, say
	\begin{equation*}
		X = \bigcup_{n = 1}^N B(y_n, r_{y_n})
	\end{equation*}
	then, by the pigeonhole principle, at least one of these balls would have to contain
	infinitely many different terms of the sequence, which is not the case. Thus \( X \) is
	not compact.

	\( (\impliedby) \) Let's now assume that \( X \) is sequentially compact. This implies
	that it is totally bounded. Indeed, if it weren't, we could find a sequence in \( X \)
	with no convergent subsequence. Let \( \epsilon > 0 \) be such that \( X \) can not be
	covered by a finite number of balls with radius \( \epsilon \) or less. Take \( x_1 \in
	X\) and consider the ball \( B(x_1, \epsilon) \). There must then exist a point \( x_2
	\in X \) outside of this ball. Since \( B(x_1, \epsilon) \cup B(x_2, \epsilon) \) cannot
	cover \( X \), there must exist \( x_3 \in X \) outside of these balls. In general,
	there must exist, for each \( n \in \N \) a point \( x_n \) such that
	\begin{equation*}
		x_n \in X - \bigcup_{k = 1}^{n-1} B(x_k, \epsilon).
	\end{equation*}
	This gives a sequence with the property that
	\begin{equation*}
		\norm{x_n - x_m} \geq \epsilon
	\end{equation*}
	when \( m \geq n \). It is easy to see that none of its subsequences is Cauchy and
	therefore cannot converge.

	Let's show now that \( X \) is compact. Consider an open cover of \( X \),
	\begin{equation*}
		X = \bigcup_{\alpha \in I} U_\alpha.
	\end{equation*}
	We have to show that we can extract from it a finite subcover. For any \( x \in X \) let 
	\begin{equation*}
		R(x) \defeq \sup \set{r > 0 \mid \exists \beta \in I \colon B(x,r) \subseteq
		U_\beta},
	\end{equation*}
	that is, \( R(x) \) is the largest possible radius such that a ball with that radius
	fits inside one of the open sets in the cover. We will show that \( \epsilon \defeq
	\inf_{x \in X} R(x) > 0 \). By definition of the infimum, there must be a sequence \( x_n
	\) in \( X \) such that \( R(x_n) \xrightarrow{n \to \infty} \epsilon \). Using the
	hypothesis of the sequential compactness of \( X \), we extract a convergent
	subsequence, \( (x_{n_k}) \), with limit \( x \). By definition of convergence, there is
	\( K \in \N \) such that
	\begin{equation*}
		x_{n_k} \in B(x, R(x))
	\end{equation*}
	provided \( k \geq K \), therefore \( R(x) \) must be strictly greater than 0, since
	otherwise \( B(x, R(x)) \) could not contain points other than \( x \). 

	% TODO: Not clear how to show that \epsilon > 0

	Since \( \epsilon > 0 \), because \( X \) is totally bounded, there must be a finite set
	of points \( y_1, \dots, y_N \in X \) such that
	\begin{equation*}
		X \subseteq \bigcup_{n = 1}^N B(y_n, \epsilon) \subseteq B(y_n, R(y_n)) \subseteq
		\bigcup_{n = 1}^N U_n.
	\end{equation*}
\end{proof}

\part{Hilbert Spaces}
\chapter{Inner and Hermitian Products}
As we mentioned earlier, Hilbert spaces are Banach spaces with the special property that
the norm they are equipped with comes from an inner product, in the real case, and a
Hermitian product in the complex case.

\section{Definition}
\begin{definition}[Inner product]
	An \emph{inner product} on a real vector space \( E \) is a map \( \phi \colon E \times
	E \to \R \) which is
	\begin{points}
	\item bilinear, that is, for all \( u, v, w \in E \) and \( \lambda, \mu \in \R \):
		\begin{gather*}
			\phi(\lambda u + \mu v, w) = \lambda \phi(u,w) + \mu \phi(v,w) \\
			\phi(u, \lambda v + \mu w) = \lambda \phi(u, v) + \mu \phi(u,w),
		\end{gather*}
		
	\item symmetric, meaning for all \( u, v \in E \)
		\begin{equation*}
			\phi(u,v) = \phi(v,u),
		\end{equation*}
		
	\item and positive definite, so that for all \( u \in E \) 
		\begin{equation*}
			\phi(u,u) \geq 0
		\end{equation*}
		and \( \phi(u,u) = 0 \) if and only if \( u = 0 \).
	\end{points}
\end{definition}

The properties of such an inner product mimic those of the standard dot product on \(
\R^n \). This gives our space a geometry that is richer than what a norm gives, as we will
see. In the complex case, the correct notion is that of a Hermitian product:
\begin{definition}
	A \emph{Hermitian product} on a complex vector space \( E \) is a map \( \phi \colon E
	\times E \to \C \) which is
	\begin{points}
	\item sesquilinear, meaning that for any \( u, v, w \in E \) and \( \lambda, \mu \in \C\)
		\begin{gather*}
			\phi(\lambda u + \mu v, w) = \lambda \phi(u,w) + \mu \phi(v,w) \\
			\phi(u, \lambda v + \mu w) = \bar{\lambda} \phi(u, v) + \bar{\mu} \phi(u,w),
		\end{gather*}
		where \( \bar{\lambda} \) is the complex conjugate of \( \lambda \),
	\item conjugate symmetric, so that for any \( u, v \in E \)
		\begin{equation}
			\phi(u,v) = \overline{\phi(v,u)},
		\end{equation}

	\item and positive definite.
	\end{points}
\end{definition}
This definition requires some remarks. Firstly, note that linearity in the first argument
and conjugate symmetry imply sesquilinearity. One often says that a Hermitian product is
\emph{linear} in its first argument and \emph{semilinear}\footnote{This just means that
scalars are conjugated when they come out} in its second\footnote{On the other hand,
physicists tend to use a different convention and assume semilinearity in the first
argument instead}. This means a Hermitian product is altogether sesquilinear, sesqui being
Latin for one and a half, as opposed to bilinear. Finally, because of conjugate
symmetry, the product of a vector with itself is always real, for a complex number is
equal to its conjugate if and only if it is real. Therefore the requirement of
positive-definiteness is sound.

It is customary to write the inner product with angle brackets, so
\begin{equation*}
	\inn{u}{v} \defeq \phi(u,v).
\end{equation*}

In what follows you should almost always have a Hermitian product in mind. In the case any
result is specialised to an inner product, so it makes use of the fact it is over the
reals, it will be duly noted. Spaces equipped with an inner or Hermitian product are called,
unsurprisingly, inner product spaces, and less often, pre-Hilbert spaces (which refers to
the fact that they are not quite product spaces, not that they were worked on before
Hilbert).

\section{The norm induced by an inner product}
An inner product induces a norm by
\begin{equation*}
	\norm{u} \defeq \sqrt{\inn{u}{u}}.
\end{equation*}
We can check it satisfies two of the properties of a norm immediately. For one, because of
positive-definiteness, \( \inn{u}{u} \) is always positive, so \( \norm{u} \) is positive
as well. And if \( \norm{u} = 0 \) then it must be \( \inn{u}{u} = 0 \), which implies \(
u = 0 \), again by positive-definiteness. 

Additionally, for any scalar \( \lambda \in \C \) one has
\begin{equation*}
	\norm{\lambda u} = \sqrt{\inn{\lambda u}{\lambda u}} = \sqrt{\lambda \bar{\lambda}
	\inn{u}{u}} = \sqrt{\abs{\lambda}^2 \inn{u}{u}} = \abs{\lambda} \sqrt{\inn{u}{u}} =
	\abs{\lambda} \norm{u}.
\end{equation*}

The proof of subadditivity does not follow as easily. It requires a very important
inequality of inner products, namely the Cauchy-Schwarz inequality.

\begin{theorem}[Cauchy-Schwarz Inequality] \label{theo:cauchy-schwarz}
	Let \( E \) be an inner product space. Then for any \( u, v \in E \)
	\begin{equation*}
		\abs{\inn{u}{v}}^2 \leq \inn{u}{u} \inn{v}{v}.
	\end{equation*}
\end{theorem}
\begin{proof}
	For any \( u, v \in E \) and \( t \in \C \) we have, by positive-definiteness
	\begin{align*}
		0 \leq \inn{u - tv}{u - tv} & = \inn{u}{u} + t\bar{t}\inn{v}{v} - t \inn{v}{u} - \bar{t}
		\inn{u}{v} \\
																& = \inn{u}{u} + \abs{t}^2\inn{v}{v} - t \inn{v}{u} - \bar{t}
		\inn{u}{v}.
	\end{align*}
	Thus
	\begin{equation*}
		t \inn{v}{u} + \bar{t} \inn{u}{v} \leq \inn{u}{u} + \abs{t}^2 \inn{v}{v}.
	\end{equation*}
	It is now a question of making an astute choice of \( t \). Letting \( t =
	\frac{\inn{u}{v}}{\inn{v}{v}} \) we find, for the LHS,
	\begin{align*}
		\frac{\inn{u}{v}}{\inn{v}{v}} \inn{v}{u} + \frac{\overline{\inn{u}{v}}}{\inn{v}{v}}
		\inn{u}{v} & = \frac{\inn{u}{v}}{\inn{v}{v}} \inn{v}{u}	+
		\frac{\inn{v}{u}}{\inn{v}{v}} \inn{u}{v} \\
							 & = \frac{2\abs{\inn{u}{v}}^2}{\inn{v}{v}}.
	\end{align*}
	Meanwhile, the RHS becomes
	\begin{equation*}
		\inn{u}{u} + \frac{\abs{\inn{u}{v}}^2}{\inn{v}{v}^2} \inn{v}{v} = \inn{u}{u} +
		\frac{\abs{\inn{u}{v}}^2}{\inn{v}{v}}.
	\end{equation*}
	Thus
	\begin{equation*}
		\frac{2\abs{\inn{u}{v}}^2}{\inn{v}{v}} \leq \inn{u}{u} +
		\frac{\abs{\inn{u}{v}}^2}{\inn{v}{v}}
	\end{equation*}
	or, equivalently,
	\begin{equation*}
		\abs{\inn{u}{v}}^2 \leq \inn{u}{u} \inn{v}{v}.
	\end{equation*}
\end{proof}

The Triangle Inequality then follows. Indeed, taking square roots in Cauchy-Schwarz (which
we can do since everything is positive, we find
\begin{equation*}
	\abs{\inn{u}{v}} \leq \sqrt{\inn{u}{u} \inn{v}{v}} = \norm{u} \norm{v}.
\end{equation*}
And from this follows
\begin{align*}
	\norm{u + v}^2 & = \inn{u + v}{u + v} \\
								 & = \inn{u}{u} + \inn{v}{v} + \inn{u}{v} + \inn{v}{u} \\
								 & = \norm{u}^2 + \norm{v}^2 + 2 \Re \inn{u}{v} \\
								 & \leq \norm{u}^2 + \norm{v}^2 + 2\abs{\inn{u}{v}} \\
								 & = \norm{u}^2 + \norm{v}^2 + 2\norm{u}\norm{v} \\
								 & = (\norm{x} + \norm{y})^2
\end{align*}
which becomes the Triangle Inequality after taking square roots. This then shows that the
Hermitian product does indeed induce a norm.

An inner product space which is complete with respect to the norm induced by its inner
product\footnote{More often than not we will simply say it is complete with respect to the
inner product} is called a \emph{Hilbert space}. A Hilbert space is in particular a Banach
space, so all of the theory developed in the previous cahpters still holds true.

\parbreak

It is possible to write the product of two vectors simply in terms of their norms, using
the so-called polarisation identities.
\begin{theorem}[Polarisation Identity] \label{theo:polarisation}
	In an inner product space \( E \) the \emph{polarisation identity}
	\begin{equation*}
		\inn{u}{v} = \frac{1}{4}\left(\norm{u + v}^2 - \norm{u - v}^2 + i\norm{u + iv}^2 - i
		\norm{u - iv}^2\right)
	\end{equation*}
	holds for any \( u, v \in E \).

	If \( E \) is a real vector space, rather than a complex vector space, then the
	polarisation identity becomes
	\begin{equation*}
		\inn{u}{v} = \frac{1}{4}\left(\norm{u + v}^2 - \norm{u - v}^2\right).
	\end{equation*}
\end{theorem}
\begin{proof}
	This is a direct computation using the definition of the norm induced by the inner
	product. Let's work out the first two terms
	\begin{gather*}
		\norm{u+v}^2 = \inn{u+v}{u+v} = \inn{u}{u} + \inn{v}{v} + \inn{u}{v} + \inn{v}{u} \\
		\norm{u-v}^2 = \inn{u-v}{u-v} = \inn{u}{u} + \inn{v}{v} - \inn{u}{v} - \inn{v}{u}.
	\end{gather*}
	Subtracting these two yields
	\begin{equation*}
		\norm{x+y}^2 - \norm{x-y}^2 = 2\inn{u}{v}	+ 2\inn{v}{u}.
	\end{equation*}
	If we have an inner product, rather than a Hermitian product, then we are done because
	of symmetry. If we don't we need to get rid of the second term, which is what the other
	two terms in the polarisation expression do:
	\begin{gather*}
		i\norm{u+iv}^2 = i\inn{u+iv}{u+iv} = i\inn{u}{u} + i\inn{v}{v} + \inn{u}{v} - \inn{v}{u} \\
		i\norm{u-iv}^2 = i\inn{u-iv}{u-iv} = i\inn{u}{u} + i\inn{v}{v} - \inn{u}{v} +
		\inn{v}{u}
	\end{gather*}
	for when we subtract we get
	\begin{equation*}
		i\norm{u+iv}^2 - i\norm{u-iv}^2 = 2 \inn{u}{v} - 2 \inn{v}{u}.
	\end{equation*}
	And then polarisation follows.
\end{proof}

Another useful identity satisfied by norms induced by an inner product is the
parallelogram law.
\begin{theorem}[Parallelogram Law]\label{theo:parallelogram}
	The following identity holds for a norm induced by an inner product:
	\begin{equation*}
		\norm{u+v}^2 + \norm{u-v}^2 = 2\norm{u}^2 + 2\norm{v}^2.
	\end{equation*}
\end{theorem}
\begin{proof}
	This is again a simple computation:
	\begin{align*}
		\norm{u+v}^2 + \norm{u-v}^2 & = \inn{u+v}{u+v} + \inn{u-v}{u-v} \\
																& = 2\inn{u}{u} + 2\inn{v}{v}+ \inn{v}{u} + \inn{u}{v} -
																\inn{v}{u} - \inn{u}{v} \\
																& = 2\norm{u}^2 + 2\norm{v}^2.
	\end{align*}
\end{proof}

This identity can be used to show that many of the normed spaces we have dealt with before
are not inner product spaces. Take \( \cont{K} \) with the uniform norm, for instance: one
can find a function \( f \in \cont{K} \) which is zero outside of a ball centered aroound
a point \( x \in K \) and such that \( \norm{f}_\infty = \abs{f(x)} = 1 \). We could then
pick another \( g \in \cont{K} \) which is zero outside of a ball centered at \( y \in
\cont{K} \) which does not intersect the ball in which \( f \) is nonzero, and such that
\( \norm{g}_\infty = \abs{f(y)} = 1 \). Then
\begin{equation*}
	\norm{f + g}_\infty + \norm{f - g}_\infty = 2 \neq 4 = 2\norm{f}_\infty + 2
	\norm{g}_\infty
\end{equation*}
which shows \( \norm{\cdot}_\infty \) cannot come from an inner product on \( \cont{K}
\). On the other hand, \( L^2(E) \) is an inner product space, for its norm is induced by
the product
\begin{equation*}
	\inn{f}{g} = \int_E fg
\end{equation*}
in the real case and 
\begin{equation*}
	\inn{f}{g} = \int_E f\bar{g}
\end{equation*}
in the real case. It's easy to see that these are indeed an inner product and a Hermitian
product, respectively.

A remarkable fact is that the converse is true, meaning a norm which satisfies the
Parallelogram Law must be induced by an inner product, and we can use the
\nameref{theo:polarisation} to recover said inner product. The proof of this is
non-trivial. As a consequence, it can be shown that the spaces \( L^p(X) \) and \( \ell^p
\) are inner product spaces exactly when \( p = 2 \).


\section{Orthogonality and orthogonal complements}
Once we have an inner product at our disposal we can define an important geometrical
notion on our space, that of orthogonality.

\begin{definition}[Orhtogonality]
	We say two non-zero vectors \( u \) and \( v \) of an inner product space \( E \) are
	orthgonal if \( \inn{u}{v} = 0 \).
\end{definition}

A generalised version of Pythagoras' Theorem holds in inner product spaces.
\begin{theorem}[Pythagoras]
	If \( u_1, \dots , u_n \in E \) are pairwise orthogonal vectors in an inner product
	space \( E \) then
	\begin{equation*}
		\norm{x_1 + \dots + x_n}^2 = \norm{x_1}^2 \dots \norm{x_n}^2.
	\end{equation*}
\end{theorem}
\begin{proof}
	Let's first show the result for \( n = 2 \). We have
	\begin{align*}
		\norm{x+y}^2 & = \inn{x+y}{x+y} \\
								 & = \inn{x}{x} + \inn{y}{y} + \inn{x}{y} + \inn{y}{x} \\
								 & = \inn{x}{x} + \inn{y}{y} \\
								 & = \norm{x}^2 + \norm{y}^2.
	\end{align*}
	
	The general case then follows by induction and by observing that if \( x_n \) is
	orthogonal to \( x_1, \dots, x_n \) then it is orthogonal to their sum.
\end{proof}

\parbreak

\begin{definition}[Orthogonal complement]
	We define the \emph{orthogonal complement} of a subset \( A \subseteq E \) of an inner
	product space \( E \) as the set of all vectors in \( E \) orthogonal to every vector in
	\( A \):
	\begin{equation*}
		A^\perp \defeq \set{x \in E \mid \forall a \in A \colon \inn{x}{a} = 0}.
	\end{equation*}
\end{definition}
Note that the set \( A \) need not be a linear subspace of \( E \). Nonetheless, the
orthogonal complement of any subset is a closed linear subspace. To prove this we make use
of the following result.

\begin{proposition}
	The inner product is continuous and for any \( x \in E \), the map
	\begin{align*}
		\inn{\cdot}{x} \colon H & \longrightarrow \C	\\
		y & \longmapsto \inn{y}{x}
	\end{align*}
	is a bounded operator with norm \( \norm{x} \).
\end{proposition}
\begin{proof}
	The continuity of the inner product follows from the following estimate:
	\begin{align*}
		\abs{\inn{x}{y} - \inn{a}{b}} & = \abs{\inn{x}{y} - \inn{a}{y} + \inn{a}{y} -
		\inn{a}{b}} \\
																	& = \abs{\inn{x-a}{y} - \inn{a}{y-b}} \\
																	& \leq \abs{\inn{x-a}{y}} - \abs{\inn{a}{y-b}} \\
																	& \leq \norm{x-a}\norm{x-a} + \norm{a}\norm{b-y}
	\end{align*}
	where we used Cauchy-Schwarz in the last step.  

	The linearity of \( \inn{\cdot}{x} \) is a consequence of the sesquilinearity of the inner
	product. The boundedness follows from Cauchy-Schwarz, since for any \( a \in E \)
	\begin{equation*}
		\abs{\inn{a}{x}} \leq \norm{x}\norm{a}
	\end{equation*}
	therefore \( \norm{\inn{\cdot}{x}} \leq \norm{x} \). And to show equality observe that,
	by definition,
	\begin{equation*}
		\abs{\inn{x}{x}} = \norm{x}^2
	\end{equation*}
	thus \( \norm{\inn{\cdot}{x}} = \norm{x} \).
\end{proof}

We can then show that the orthogonal complement satisfies the following useful
properties.
{\def\currentprefix{prop:properties of perp}
\begin{proposition}
	Let \( E \) be an inner product space. Then the follwoing are true
	\begin{points}
	\item \locallabel{i} If \( A \subseteq B \subseteq E \) are any two subsets then
		\begin{equation*}
			B^\perp \subseteq A^\perp.
		\end{equation*}

	\item \locallabel{ii} The orthogonal complement of any subset of \( E \) is a closed
		linear subspace of \( E \).
	\end{points}
\end{proposition}
\begin{proof}
	\localref{i} If \( x \in B^\perp \) then \( x \) is orthogonal to every element of \( B
	\), which means, in particular, it is orthogonal to any element of \( A \). Thus \( x
	\in A^\perp \). 
	\localref{ii} If \( x, y \in A^\perp \) then, for any \( \lambda, \mu \in \R \) and \( a
	\in A \)
	\begin{equation*}
		\inn{\lambda x + \mu y}{a} = \lambda \inn{x}{a} + \mu \inn{y}{a} = 0
	\end{equation*}
	so \( \lambda x + \mu y \in A^\perp \). Since \( 0 \in A^\perp \) we conclude \( A^\perp
	\) is a linear subspace. To show it is closed, consider a sequence \( (x_n) \) in \(
	A^\perp \) which converges to \( x \in E \). If we can show \( x \in A^\perp \) then we
	are done. But this follows from the continuity of the inner product we just proved: indeed, for any \(
	a \in A \)
	\begin{equation*}
		\inn{x}{a} = \inn{\lim_{n \to \infty}}{a} = \lim_{n \to \infty} \inn{x_n}{a} = \lim_{n
		\to \infty} 0 = 0 
	\end{equation*}
	so \( x \in A^\perp \).

	Alternatively, observe that 
	\begin{equation*}
		A^\perp = \bigcap_{a \in A} \inn{\cdot}{a}^{-1}(\set{0}).
	\end{equation*}
	As we proved before, the linear form \( \inn{\cdot}{a} \) is bounded, therefore continuous,
	thus \( \inn{\cdot}{a}^{-1}(\set{0}) \) is closed since \( \set{0} \). Thus \( A^\perp
	\) is also closed since it is the intersection of closed sets.
\end{proof}
}

\chapter{Projection onto subspaces}
\section{Existence and uniqueness of optimal approximations}
Recall that given a closed subset \( A \) of a normed space\footnote{We could consider
this for metric spaces if we wanted to be extra general but normed spaces suffice}, so in
particular of an inner product space \( H \), we can talk of the distance between a point
\( x \in H \) and \( A \), which is
\begin{equation*}
	d(x,A) = \inf_{a \in A} \norm{x - a}.
\end{equation*}
The closedness of \( A \) is necessary if we want the property that \( d(x,A) = 0 \) if
and only if \( x \in A \). We have, then, that \( d(x,A) \geq 0 \) provided \( x \notin A
\). This does not mean, however, that there exists \( a \in A \) such that \( \norm{x - a}
= d(x,A) \). If this is the case, though, we say \( a \) is an \emph{optimal
approximation} of \( x \) in \( A \). This means \( a \) is the closest we will ever get
to \( x \) if we restrict ourselves to \( A \).

In this section we will investigate the existence and uniqueness of optimal approximations
depending on the properties of \( A \).

\begin{proposition}
	If \( A \subseteq H \) is a closed and convex\footnote{Recall that a subset \( A \) of a
	vector space is convex if, for any two points \( a \) and \( b \) in \( A \) then the
segment between them is also in	\( A \), and the segment between \( a \) and \( b \) is
the set of all linear combinations of \( a \) and \( b \) such that their coefficients add
up to 1.} and \( x \) has an optimal approximation in	\( A \) then it is unique.
\end{proposition}
\begin{proof}
	Let \( y, z \in A \) be optimal approximations of \( x \) in \( A \). To prove
	uniqueness we need to show that \( y = z \). By definition of optimal approximation, we
	have
	\begin{equation*}
		\norm{y - x} + \norm{z - x} = d(x,A) > 0,
	\end{equation*}
	the last inequality because \( A \) is closed. 

	The idea of the proof is to use the fact that if two points \( y \) and \( z \)  are
	equidistant from a third point \( x \), then any other point in the segment between \( y
	\) and \( z \) is closer to \( x \) than \( y \) and \( z \). Then, if \( y \) and \( z \) are
	optimal approximations of \( x \) then, because of the convexity of \( A \), there must
	be other points in \( A \) which are closer to \( x \). This cannot be the case unless
	the segment between \( z \) and \( y \) contains no points other than \( z \) and \( y
	\) themselves, which implies \( y \) and \( z \) are equal.

	Let's write this out. By virtue of \( A \) being convex, the point \( \frac{y+z}{2} \)
	lies in	\( A \). This means
	\begin{equation*}
		\norm{\frac{y+z}{2} - x} \geq d(x,A).
	\end{equation*}
	
	On the other hand, using the \nameref{theo:parallelogram} we have
	\begin{equation*}
		2\norm{y-x}^2 + 2\norm{z-x}^2  = \norm{y-z}^2 + \norm{y+z - 2x^2} = \norm{y-z}^2 +
		4\norm{\frac{y+z}{2} - x}^2.
	\end{equation*}
	Since \( \norm{y-x}^2 = \norm{z-x}^2 = d(x,A)^2 \), we have
	\begin{equation*}
		4\norm{\frac{y+z}{2} - x}^2 = 4d(x,A)^2 - \norm{y - z}^2.
	\end{equation*}
	But using \( \norm{\frac{y+z}{2} - x} \geq d(x,A) \) one finds \( \norm{y - z} = 0 \),
	which means, of course, that \( y = z \), as we wanted.
\end{proof}
The requirement that \( A \) be convex is important. Indeed, consider the unit circle \(
S^1 \) in \( \R^2 \). Then every point of \( S^1 \) is distance 1 from the origin, thus
they all are optimal approximations of it. \( S^1 \) is not convex, however.

When \( x \) has a unique optimal approximation in a set \( A \) we write it \( P_A x \)
and call it its \emph{projection onto \( A \)}.
\begin{theorem}
	Let \( H \) be a Hilbert space and \( A \subseteq H \) a subset. The map
	\begin{align*}
		P_A \colon H & \longrightarrow A \\
		x & \longmapsto P_A x
	\end{align*}
	is well-defined provided \( A \) is closed and convex.
\end{theorem}
\begin{proof}
	All that remains to be shown is that an optimal approximation in \( A \) exists, since
	the previous proposition shows that it is unique if it does exist. For this we need to
	use the fact that \( A \) is closed. 

	Since \( d(x,A) \) is defined as an infimum, there exists a sequence \( y_n \) in \( A
	\) such that \( \norm{y_n - a} \xrightarrow{n \to \infty} d(x,A) \). Using the
	\nameref{theo:parallelogram} we find
	\begin{align*}
 		\norm{y_n - y_m}^2 + 4\norm{\frac{y_n + y_m}{2} - x}^2 & = \norm{y_n - y_m}^2 +
 		\norm{y_n + y_m - 2x}^2 \\
 																													 & = 2\norm{y_n - x}^2 +
 																													 2\norm{y_m - x}^2.
	\end{align*}
	Because \( A \) is convex, \( \frac{y_n + y_m}{2} \in A \) and thus
	\begin{equation*}
		\norm{\frac{y_n + y_m}{2} - x} \geq d(x,A).
	\end{equation*}
	This means
	\begin{align*}
		\norm{y_n - y_m}^2 & \leq 2\norm{y_n - x}^2 + 2\norm{y_m - x}^2 - 4d(x,A)^2 \\
											 & = 2(\norm{y_n - x}^2 - d(x,A)^2) + 2(\norm{y_m - x}^2 -
											 d(x,A)^2). 
	\end{align*}
	And now just use the fact that \( \norm{y_n - x} \) converges to \( d(x,A) \) to
	conclude \( y_n \) is Cauchy. Because \( H \) is a Hilbert space it is complete and
	therefore \( y_n \) converges to \( y \in H \). But because \( A \) is closed, in fact
	\( y \in A \). And, thus
	\begin{equation*}
		d(x,A) = \lim_{n \to \infty} \norm{y_n - x} = \norm{\lim_{n \to \infty} y_n - a} =
		\norm{y - a}.
	\end{equation*}
	It follows, then, that \( y = P_Ax \).
\end{proof}
\begin{corollary}
	The projection onto a closed linear subspace of a Hilbert space is well defined.
\end{corollary}
\begin{proof}
	This follows from the fact that a linear subspace is convex.
\end{proof}

For the case of linear subspaces, we can give a useful characterisation of the projection
in terms of the orthogonal complement.
\begin{proposition}
	Let \( F \subseteq H \) be a closed linear subspace of a Hilbert space. Then \( y \in F
	\) is the projection of \( x \in H \) onto \( F \) if and only if \( x - y \in F^\perp
	\).
\end{proposition}
\begin{proof}
	\( (\implies) \) Say \( y \in F \) is such that \( y - x \in F^\perp \). Then, for any
	other \( z \in F \) we have \( \inn{x - y}{y - z} = 0 \). Then, since \( x - z = x-y +
	y-z \) use Pythagoras to conclude
	\begin{equation*}
		\norm{x - z}^2 = \norm{x-y}^2 + \norm{y-z}^2 \geq \norm{x-y}^2.
	\end{equation*}
	This means \( \norm{x-y} \leq \inf_{z \in F} \norm{x - z} \). But since \( y \in F \) we
	conclude
	\begin{equation*}
		\norm{x-y} = \inf_{z \in F}\norm{x-z} = d(x,F)
	\end{equation*}
	which means \( y = P_Fx \).

	\( (\impliedby) \) The proof of the converse is a bit more involved. Suppose \( y =
	P_Fx \). This means \( \norm{y-x} = d(x,F) \). We need to show \( \inn{y-x}{z} = 0 \)
	for all \( z \in F \). Since \( y = P_F x \) we have, for all \( \lambda \in \C \) and
	\( z \in F \)
	\begin{equation*}
		\norm{x - (y+\lambda z)}^2 \geq \norm{x-y}^2.
	\end{equation*}
	Then
	\begin{align*}
		\norm{x-y}^2 & \leq \norm{x-(y + \lambda z)}^2 = \inn{x-(y + \lambda z)}{x-(y +
		\lambda z)} \\
								 & = \norm{x-y}^2 + \norm{\lambda z}^2 - \inn{x-y}{\lambda z} -
								 \inn{\lambda z}{x-y} \\
								 & = \norm{x-y}^2 + \norm{\lambda z}^2 - 2 \Re\left(\lambda
								 \inn{z}{x-y}\right).
	\end{align*}
	Cancelling \( \norm{x-y}^2 \) and rearranging, this is equivalent to
	\begin{equation*}
		\abs{\lambda z}^2 \norm{z}^2 \geq 2\Re\left(\lambda \inn{z}{x-y}\right).
	\end{equation*}
	Let's pick \( \lambda = s \inn{x-y}{z} \) with \( s \in \R \). Then this means
	\begin{equation*}
		s^2 \norm{z}^2 \abs{\inn{x-y}{z}}^2 \geq 2s\abs{\inn{x-y}{z}}^2.
	\end{equation*}
	Now, since \( \frac{cs^2}{2s} \xrightarrow{s \to 0} \) for any \( c > 0 \) we can find
	\( s \in \R \) such that \( 2s \leq \norm{z}^2s^2 \) (why?). This must mean \(
	\inn{x-y}{z} = 0 \) since otherwise the inequality could not possibly hold for any \( s
	\in \R \). Therefore \( x-y \in F^\perp \), as we wished.
\end{proof}

All of these facts lead to the following important result in the theory of Hilbert
spaces.
\begin{theorem}[Projection Theorem]\label{theo:projection}
	Given any closed linear subspace \( F \) of a Hilbert space \( H \) then for any \( x
	\in H \)
	\begin{equation*}
		P_F x + P_{F^\perp} x = x.
	\end{equation*}
\end{theorem}
\begin{proof}
	First, since both \( F \) and \( F^\perp \) are closed linear subspaces, the projections
	onto both are well defined. If we show \( P_{F^\perp}x = x - P_F x \) then we are done.
	For any \( y \in F^\perp \) we have
	\begin{equation*}
		\norm{x -y}^2 = \norm{x - P_Fx + P_Fx - y}^2 
	\end{equation*}
	We showed \( x - P_Fx \in F^\perp \), which means \( x-P_Fx - y \) is also in \( F^\perp
	\), thus
	\begin{equation*}
		\inn{P_F x}{x - P_Fx - y} = 0,
	\end{equation*}
	 which in turns implies, by Pythagoras
	\begin{equation*}
		\norm{x -y}^2 = \norm{P_Fx}^2 + \norm{x-P_Fx-y}^2 \geq \norm{P_Fx}^2 = \norm{x -
		(x-P_Fx)}^2.
	\end{equation*}
	This lets conclude \( P_F^\perp = x - P_Fx \). 
\end{proof}
\begin{corollary}
	Any Hilbert space \( H \) can be decomposed as
	\begin{equation*}
		H = F \oplus F^\perp
	\end{equation*}
	where \( F \) is a closed linear subspace of \( H \).
\end{corollary}
\begin{proof}
	By the \nameref{theo:projection}, we have \( H = F + F^\perp \), so all we need to show
	is \( F \cap F^\perp = \gen{0} \). And this is true. Indeed, if \( x \in F \cap F^\perp
	\) then, in particular, \( \inn{x}{x} = 0 \). And this means \( x = 0 \), by
	positive-definiteness.
\end{proof}

\end{document}

